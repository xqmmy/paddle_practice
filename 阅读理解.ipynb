{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 基于预训练模型的机器阅读理解\n",
    "\n",
    "阅读理解是检索问答系统中的重要组成部分，最常见的数据集是单篇章、**抽取式**阅读理解数据集。  \n",
    "\n",
    "该示例展示了如何使用PaddleNLP快速实现基于预训练模型的机器阅读理解任务。\n",
    "\n",
    "本示例使用的数据集是Dureader<sub>robust</sub>数据集。对于一个给定的问题q和一个篇章p，根据篇章内容，给出该问题的答案a。数据集中的每个样本，是一个三元组<q, p, a>，例如：\n",
    "\n",
    "**问题 q**: 乔丹打了多少个赛季\n",
    "\n",
    "**篇章 p**: 迈克尔.乔丹在NBA打了15个赛季。他在84年进入nba，期间在1993年10月6日第一次退役改打棒球，95年3月18日重新回归，在99年1月13日第二次退役，后于2001年10月31日复出，在03年最终退役…\n",
    "\n",
    "**参考答案 a**: [‘15个’,‘15个赛季’]\n",
    "\n",
    "阅读理解模型的鲁棒性是衡量该技术能否在实际应用中大规模落地的重要指标之一。随着当前技术的进步，模型虽然能够在一些阅读理解测试集上取得较好的性能，但在实际应用中，这些模型所表现出的鲁棒性仍然难以令人满意。\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/67bb14fa9db64908ba03288ea1bcd49fb523f77f8adf43129293411b89a2f2a4)\n",
    "\n",
    "\n",
    "\n",
    "本示例使用的Dureader<sub>robust</sub>数据集作为首个关注阅读理解模型鲁棒性的中文数据集，旨在考察模型在真实应用场景中的过敏感性、过稳定性以及泛化能力等问题。\n",
    "\n",
    "关于该数据集的详细内容，可参考数据集[论文](https://arxiv.org/abs/2004.11142)，或官方[比赛链接](https://aistudio.baidu.com/aistudio/competition/detail/49?castk=LTE=)。\n",
    "\n",
    "## 安装说明\n",
    "\n",
    "* PaddlePaddle 安装\n",
    "\n",
    "   本项目依赖于 PaddlePaddle 2.0 及以上版本，请参考 [安装指南](https://www.paddlepaddle.org.cn/install/quick?docurl=/documentation/docs/zh/install/pip/windows-pip.html) 进行安装\n",
    "\n",
    "* PaddleNLP 安装\n",
    "\n",
    "   ```shell\n",
    "   pip install --upgrade paddlenlp -i https://pypi.org/simple\n",
    "   ```\n",
    "\n",
    "* 环境依赖\n",
    "\n",
    "    Python的版本要求 3.6+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "AI Studio平台后续会默认安装PaddleNLP，在此之前可使用如下命令安装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting paddlenlp\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/63/7a/e6098c8794d7753470071f58b07843824c40ddbabe213eae458d321d2dbe/paddlenlp-2.0.3-py3-none-any.whl (451kB)\n",
      "\u001b[K     |████████████████████████████████| 460kB 25kB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: colorlog in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (4.1.0)\n",
      "Requirement already satisfied, skipping upgrade: visualdl in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (2.1.1)\n",
      "Requirement already satisfied, skipping upgrade: h5py in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (2.9.0)\n",
      "Requirement already satisfied, skipping upgrade: jieba in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (0.42.1)\n",
      "Requirement already satisfied, skipping upgrade: colorama in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (0.4.4)\n",
      "Requirement already satisfied, skipping upgrade: multiprocess in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (0.70.11.1)\n",
      "Requirement already satisfied, skipping upgrade: seqeval in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (1.2.2)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.14.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: flake8>=3.7.9 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (3.8.2)\n",
      "Requirement already satisfied, skipping upgrade: flask>=1.1.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (1.1.1)\n",
      "Requirement already satisfied, skipping upgrade: pre-commit in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (1.21.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (1.20.3)\n",
      "Requirement already satisfied, skipping upgrade: Pillow>=7.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (7.1.2)\n",
      "Requirement already satisfied, skipping upgrade: bce-python-sdk in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (0.8.53)\n",
      "Requirement already satisfied, skipping upgrade: shellcheck-py in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (0.7.1.1)\n",
      "Requirement already satisfied, skipping upgrade: Flask-Babel>=1.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (1.0.0)\n",
      "Requirement already satisfied, skipping upgrade: requests in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (2.22.0)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.11.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (3.14.0)\n",
      "Requirement already satisfied, skipping upgrade: dill>=0.3.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from multiprocess->paddlenlp) (0.3.3)\n",
      "Requirement already satisfied, skipping upgrade: scikit-learn>=0.21.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from seqeval->paddlenlp) (0.24.2)\n",
      "Requirement already satisfied, skipping upgrade: mccabe<0.7.0,>=0.6.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl->paddlenlp) (0.6.1)\n",
      "Requirement already satisfied, skipping upgrade: pycodestyle<2.7.0,>=2.6.0a1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl->paddlenlp) (2.6.0)\n",
      "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl->paddlenlp) (0.23)\n",
      "Requirement already satisfied, skipping upgrade: pyflakes<2.3.0,>=2.2.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl->paddlenlp) (2.2.0)\n",
      "Requirement already satisfied, skipping upgrade: itsdangerous>=0.24 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl->paddlenlp) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: Werkzeug>=0.15 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl->paddlenlp) (0.16.0)\n",
      "Requirement already satisfied, skipping upgrade: Jinja2>=2.10.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl->paddlenlp) (2.10.1)\n",
      "Requirement already satisfied, skipping upgrade: click>=5.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl->paddlenlp) (7.0)\n",
      "Requirement already satisfied, skipping upgrade: virtualenv>=15.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (16.7.9)\n",
      "Requirement already satisfied, skipping upgrade: toml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (0.10.0)\n",
      "Requirement already satisfied, skipping upgrade: identify>=1.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (1.4.10)\n",
      "Requirement already satisfied, skipping upgrade: nodeenv>=0.11.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (1.3.4)\n",
      "Requirement already satisfied, skipping upgrade: aspy.yaml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (1.3.0)\n",
      "Requirement already satisfied, skipping upgrade: pyyaml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (5.1.2)\n",
      "Requirement already satisfied, skipping upgrade: cfgv>=2.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (2.0.1)\n",
      "Requirement already satisfied, skipping upgrade: future>=0.6.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from bce-python-sdk->visualdl->paddlenlp) (0.18.0)\n",
      "Requirement already satisfied, skipping upgrade: pycryptodome>=3.8.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from bce-python-sdk->visualdl->paddlenlp) (3.9.9)\n",
      "Requirement already satisfied, skipping upgrade: Babel>=2.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Flask-Babel>=1.0.0->visualdl->paddlenlp) (2.8.0)\n",
      "Requirement already satisfied, skipping upgrade: pytz in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Flask-Babel>=1.0.0->visualdl->paddlenlp) (2019.3)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl->paddlenlp) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl->paddlenlp) (2019.9.11)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl->paddlenlp) (1.25.6)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl->paddlenlp) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.19.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp) (1.6.3)\n",
      "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp) (0.14.1)\n",
      "Requirement already satisfied, skipping upgrade: threadpoolctl>=2.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp) (2.1.0)\n",
      "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->flake8>=3.7.9->visualdl->paddlenlp) (0.6.0)\n",
      "Requirement already satisfied, skipping upgrade: MarkupSafe>=0.23 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Jinja2>=2.10.1->flask>=1.1.1->visualdl->paddlenlp) (1.1.1)\n",
      "Requirement already satisfied, skipping upgrade: more-itertools in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from zipp>=0.5->importlib-metadata; python_version < \"3.8\"->flake8>=3.7.9->visualdl->paddlenlp) (7.2.0)\n",
      "Installing collected packages: paddlenlp\n",
      "  Found existing installation: paddlenlp 2.0.1\n",
      "    Uninstalling paddlenlp-2.0.1:\n",
      "      Successfully uninstalled paddlenlp-2.0.1\n",
      "Successfully installed paddlenlp-2.0.3\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade paddlenlp -i https://pypi.org/simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 示例流程\n",
    "\n",
    "与大多数NLP任务相同，本次机器阅读理解任务的示例展示分为以下四步：\n",
    "\n",
    "首先我们从数据准备开始。\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/dd30e17318fb48fabb5701fd8a97be8176a1e372dd134cc0826e58cb5401933d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 数据准备\n",
    "\n",
    "数据准备流程如下：\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/127ff46bf5c342889991438a95aa7158ec06c8852e58410ba7a516db330175f2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 1. 加载PaddleNLP内置数据集\n",
    "\n",
    "使用PaddleNLP提供的`load_dataset`API，即可一键完成数据集加载。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'paddlenlp.datasets.DuReader_yesno'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-e4649a05ea32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpaddlenlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'DuReader_yesno'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dev'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddlenlp/datasets/dataset.py\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(path_or_read_func, name, data_files, splits, lazy, **kwargs)\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mreader_instance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mcustom_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mreader_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimport_main_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_read_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0mreader_instance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreader_cls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlazy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlazy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddlenlp/datasets/dataset.py\u001b[0m in \u001b[0;36mimport_main_class\u001b[0;34m(module_path)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \"\"\"\n\u001b[1;32m     45\u001b[0m     \u001b[0mmodule_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDATASETS_MODULE_PATH\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0mmain_cls_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDatasetBuilder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'paddlenlp.datasets.DuReader_yesno'"
     ]
    }
   ],
   "source": [
    "from paddlenlp.datasets import load_dataset\r\n",
    "\r\n",
    "train_ds, dev_ds, test_ds = load_dataset('dureader_robust', splits=('train', 'dev', 'test'))\r\n",
    "\r\n",
    "for idx in range(2):\r\n",
    "    print(train_ds[idx]['question'])\r\n",
    "    print(train_ds[idx]['context'])\r\n",
    "    print(train_ds[idx]['answers'])\r\n",
    "    print(train_ds[idx]['answer_starts'])\r\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "关于更多PaddleNLP数据集，请参考[数据集列表](https://paddlenlp.readthedocs.io/zh/latest/data_prepare/dataset_list.html)\n",
    "\n",
    "如果你想使用自己的数据集文件构建数据集，请参考[以内置数据集格式读取本地数据集](https://paddlenlp.readthedocs.io/zh/latest/data_prepare/dataset_load.html#id4)和[自定义数据集](https://paddlenlp.readthedocs.io/zh/latest/data_prepare/dataset_self_defined.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2. 加载 `paddlenlp.transformers.ErnieTokenizer`用于数据处理\n",
    "\n",
    "DuReader<sub>rubust</sub>数据集采用SQuAD数据格式，InputFeature使用滑动窗口的方法生成，即一个example可能对应多个InputFeature。\n",
    "\n",
    "由于文章加问题的文本长度可能大于max_seq_length，答案出现的位置有可能出现在文章最后，所以不能简单的对文章进行截断。\n",
    "\n",
    "那么对于过长的文章，则采用滑动窗口将文章分成多段，分别与问题组合。再用对应的tokenizer转化为模型可接受的feature。doc_stride参数就是每次滑动的距离。滑动窗口生成InputFeature的过程如下图：\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/5776cf9ec00546bca047a0930c8f56a8b64d723e0ff04f269334522954bb7d90)\n",
    "\n",
    "本基线中，我们使用的预训练模型是ERNIE，ERNIE对中文数据的处理是以字为单位。**PaddleNLP对于各种预训练模型已经内置了相应的tokenizer**，指定想要使用的模型名字即可加载对应的tokenizer。\n",
    "\n",
    "tokenizer的作用是将原始输入文本转化成模型可以接受的输入数据形式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-06-24 09:33:34,608] [    INFO] - Downloading vocab.txt from https://paddlenlp.bj.bcebos.com/models/transformers/ernie/vocab.txt\n",
      "100%|██████████| 90/90 [00:00<00:00, 5666.02it/s]\n"
     ]
    }
   ],
   "source": [
    "import paddlenlp\n",
    "\n",
    "# 设置模型名称\n",
    "MODEL_NAME = 'ernie-1.0'\n",
    "tokenizer = paddlenlp.transformers.ErnieTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3. 调用`map()`方法批量处理数据\n",
    "\n",
    "由于我们传入了`lazy=False`，所以我们使用`load_dataset()`自定义的数据集是`MapDataset`对象。`MapDataset`是`paddle.io.Dataset`的功能增强版本。其内置的`map()`方法适合用来进行批量数据集处理。\n",
    "\n",
    "`map()`方法接受的主要参数是一个用于数据处理的function。正好可以与tokenizer相配合。\n",
    "\n",
    "以下是本示例中的用法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<paddlenlp.datasets.dataset.MapDataset at 0x7fe2f50cb210>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import prepare_train_features, prepare_validation_features\r\n",
    "from functools import partial\r\n",
    "\r\n",
    "max_seq_length = 512\r\n",
    "doc_stride = 128\r\n",
    "\r\n",
    "train_trans_func = partial(prepare_train_features, \r\n",
    "                           max_seq_length=max_seq_length, \r\n",
    "                           doc_stride=doc_stride,\r\n",
    "                           tokenizer=tokenizer)\r\n",
    "\r\n",
    "train_ds.map(train_trans_func, batched=True, num_workers=4)\r\n",
    "\r\n",
    "dev_trans_func = partial(prepare_validation_features, \r\n",
    "                           max_seq_length=max_seq_length, \r\n",
    "                           doc_stride=doc_stride,\r\n",
    "                           tokenizer=tokenizer)\r\n",
    "                           \r\n",
    "dev_ds.map(dev_trans_func, batched=True, num_workers=4)\r\n",
    "test_ds.map(dev_trans_func, batched=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1034, 1189, 734, 2003, 241, 284, 131, 553, 271, 28, 125, 280, 2, 131, 1773, 271, 1097, 373, 1427, 1427, 501, 88, 662, 1906, 4, 561, 125, 311, 1168, 311, 692, 46, 430, 4, 84, 2073, 14, 1264, 3967, 5, 1034, 1020, 1829, 268, 4, 373, 539, 8, 154, 5210, 4, 105, 167, 59, 69, 685, 12043, 539, 8, 883, 1020, 4, 29, 720, 95, 90, 427, 67, 262, 5, 384, 266, 14, 101, 59, 789, 416, 237, 12043, 1097, 373, 616, 37, 1519, 93, 61, 15, 4, 255, 535, 7, 1529, 619, 187, 4, 62, 154, 451, 149, 12043, 539, 8, 253, 223, 3679, 323, 523, 4, 535, 34, 87, 8, 203, 280, 1186, 340, 9, 1097, 373, 5, 262, 203, 623, 704, 12043, 84, 2073, 1137, 358, 334, 702, 5, 262, 203, 4, 334, 702, 405, 360, 653, 129, 178, 7, 568, 28, 15, 125, 280, 518, 9, 1179, 487, 12043, 84, 2073, 1621, 1829, 1034, 1020, 4, 539, 8, 448, 91, 202, 466, 70, 262, 4, 638, 125, 280, 83, 299, 12043, 539, 8, 61, 45, 7, 1537, 176, 4, 84, 2073, 288, 39, 4, 889, 280, 14, 125, 280, 156, 538, 12043, 190, 889, 280, 71, 109, 124, 93, 292, 889, 46, 1248, 4, 518, 48, 883, 125, 12043, 539, 8, 268, 889, 280, 109, 270, 4, 1586, 845, 7, 669, 199, 5, 3964, 3740, 1084, 4, 255, 440, 616, 154, 72, 71, 109, 12043, 49, 61, 283, 3591, 34, 87, 297, 41, 9, 1993, 2602, 518, 52, 706, 109, 12043, 37, 10, 561, 125, 43, 8, 445, 86, 576, 65, 1448, 2969, 4, 469, 1586, 118, 776, 5, 1993, 2602, 4, 108, 25, 179, 51, 1993, 2602, 498, 1052, 122, 12043, 1082, 1994, 1616, 11, 262, 4, 518, 171, 813, 109, 1084, 270, 12043, 539, 8, 3006, 580, 11, 31, 4, 2473, 306, 34, 87, 889, 280, 846, 573, 12043, 561, 125, 14, 539, 889, 810, 276, 182, 4, 67, 351, 14, 889, 1182, 118, 776, 156, 952, 4, 539, 889, 16, 38, 4, 445, 15, 200, 61, 12043, 2]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "0\n",
      "[(0, 0), (0, 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, 6), (6, 7), (7, 8), (8, 9), (9, 10), (10, 11), (11, 12), (0, 0), (0, 1), (1, 3), (3, 4), (4, 5), (5, 6), (6, 7), (7, 8), (8, 9), (9, 10), (10, 11), (11, 12), (12, 13), (13, 14), (14, 15), (15, 16), (16, 17), (17, 18), (18, 19), (19, 20), (20, 21), (21, 22), (22, 23), (23, 24), (24, 25), (25, 26), (26, 27), (27, 28), (28, 29), (29, 30), (30, 31), (31, 32), (32, 33), (33, 34), (34, 35), (35, 36), (36, 37), (37, 38), (38, 39), (39, 40), (40, 41), (41, 42), (42, 43), (43, 44), (44, 45), (45, 46), (46, 47), (47, 48), (48, 49), (49, 50), (50, 51), (51, 52), (52, 53), (53, 54), (54, 55), (55, 56), (56, 57), (57, 58), (58, 59), (59, 60), (60, 61), (61, 62), (62, 63), (63, 64), (64, 65), (65, 66), (66, 67), (67, 68), (68, 69), (69, 70), (70, 71), (71, 72), (72, 73), (73, 74), (74, 75), (75, 76), (76, 77), (77, 78), (78, 79), (79, 80), (80, 81), (81, 82), (82, 83), (83, 84), (84, 85), (85, 86), (86, 87), (87, 88), (88, 89), (89, 90), (90, 91), (91, 92), (92, 93), (93, 94), (94, 95), (95, 96), (96, 97), (97, 98), (98, 99), (99, 100), (100, 101), (101, 102), (102, 103), (103, 104), (104, 105), (105, 106), (106, 107), (107, 108), (108, 109), (109, 110), (110, 111), (111, 112), (112, 113), (113, 114), (114, 115), (115, 116), (116, 117), (117, 118), (118, 119), (119, 120), (120, 121), (121, 122), (122, 123), (123, 124), (124, 125), (125, 126), (126, 127), (127, 128), (128, 129), (129, 130), (130, 131), (131, 132), (132, 133), (133, 134), (134, 135), (135, 136), (136, 137), (137, 138), (138, 139), (139, 140), (140, 141), (141, 142), (142, 143), (143, 144), (144, 145), (145, 146), (146, 147), (147, 148), (148, 149), (149, 150), (150, 151), (151, 152), (152, 153), (153, 154), (154, 155), (155, 156), (156, 157), (157, 158), (158, 159), (159, 160), (160, 161), (161, 162), (162, 163), (163, 164), (164, 165), (165, 166), (166, 167), (167, 168), (168, 169), (169, 170), (170, 171), (171, 172), (172, 173), (173, 174), (174, 175), (175, 176), (176, 177), (177, 178), (178, 179), (179, 180), (180, 181), (181, 182), (182, 183), (183, 184), (184, 185), (185, 186), (186, 187), (187, 188), (188, 189), (189, 190), (190, 191), (191, 192), (192, 193), (193, 194), (194, 195), (195, 196), (196, 197), (197, 198), (198, 199), (199, 200), (200, 201), (201, 202), (202, 203), (203, 204), (204, 205), (205, 206), (206, 207), (207, 208), (208, 209), (209, 210), (210, 211), (211, 212), (212, 213), (213, 214), (214, 215), (215, 216), (216, 217), (217, 218), (218, 219), (219, 220), (220, 221), (221, 222), (222, 223), (223, 224), (224, 225), (225, 226), (226, 227), (227, 228), (228, 229), (229, 230), (230, 231), (231, 232), (232, 233), (233, 234), (234, 235), (235, 236), (236, 237), (237, 238), (238, 239), (239, 240), (240, 241), (241, 242), (242, 243), (243, 244), (244, 245), (245, 246), (246, 247), (247, 248), (248, 249), (249, 250), (250, 251), (251, 252), (252, 253), (253, 254), (254, 255), (255, 256), (256, 257), (257, 258), (258, 259), (259, 260), (260, 261), (261, 262), (262, 263), (263, 264), (264, 265), (265, 266), (266, 267), (267, 268), (268, 269), (269, 270), (270, 271), (271, 272), (272, 273), (273, 274), (274, 275), (275, 276), (276, 277), (277, 278), (278, 279), (279, 280), (280, 281), (281, 282), (282, 283), (283, 284), (284, 285), (285, 286), (286, 287), (287, 288), (288, 289), (289, 290), (290, 291), (291, 292), (292, 293), (293, 294), (294, 295), (295, 296), (296, 297), (297, 298), (298, 299), (299, 300), (300, 301), (301, 302), (302, 303), (303, 304), (304, 305), (305, 306), (306, 307), (307, 308), (308, 309), (309, 310), (310, 311), (311, 312), (312, 313), (313, 314), (314, 315), (315, 316), (316, 317), (317, 318), (318, 319), (319, 320), (320, 321), (321, 322), (322, 323), (323, 324), (324, 325), (325, 326), (326, 327), (327, 328), (328, 329), (329, 330), (330, 331), (331, 332), (0, 0)]\n",
      "14\n",
      "16\n",
      "\n",
      "[1, 1404, 266, 506, 101, 361, 1256, 27, 664, 85, 170, 2, 352, 790, 1404, 266, 506, 101, 361, 36, 4, 7, 91, 41, 129, 490, 47, 553, 27, 358, 281, 74, 208, 6, 39, 101, 862, 91, 92, 41, 170, 4, 16, 52, 39, 87, 1745, 506, 1745, 888, 5, 87, 528, 249, 6, 532, 537, 45, 302, 94, 91, 5, 413, 323, 101, 565, 284, 6, 868, 25, 41, 826, 52, 6, 58, 518, 397, 6, 204, 62, 92, 41, 170, 4, 41, 371, 9, 204, 62, 337, 1023, 371, 521, 99, 191, 28, 1404, 266, 506, 101, 361, 100, 664, 539, 65, 4, 817, 1042, 36, 201, 413, 65, 120, 51, 277, 14, 2081, 541, 1190, 348, 12043, 58, 512, 508, 17, 57, 445, 5, 1512, 73, 1664, 565, 506, 101, 361, 11, 175, 29, 82, 412, 58, 76, 388, 15, 62, 76, 658, 222, 74, 701, 1866, 537, 506, 4, 48, 532, 537, 71, 109, 1123, 1600, 469, 220, 12048, 101, 565, 303, 876, 862, 91, 4, 16, 32, 39, 87, 1745, 506, 1745, 888, 5, 87, 528, 4, 145, 124, 93, 101, 150, 3466, 231, 164, 133, 174, 39, 101, 565, 130, 326, 524, 586, 108, 11, 17963, 42, 17963, 4, 48, 596, 581, 50, 155, 707, 1358, 1443, 345, 1455, 1411, 1123, 455, 413, 323, 12048, 483, 366, 4850, 14, 6215, 9488, 653, 266, 82, 337, 1023, 371, 521, 263, 204, 62, 78, 99, 191, 28, 7, 689, 65, 13, 4850, 269, 266, 82, 337, 1023, 77, 12043, 770, 137, 4, 47, 699, 506, 101, 361, 201, 9, 826, 52, 4177, 756, 387, 369, 52, 4, 297, 413, 86, 763, 27, 247, 98, 3887, 444, 48, 29, 247, 98, 629, 163, 868, 25, 506, 101, 361, 4, 79, 87, 326, 378, 290, 377, 101, 565, 4, 596, 581, 50, 8, 65, 314, 73, 5, 1123, 1600, 413, 323, 12043, 153, 187, 58, 512, 5, 1512, 73, 1664, 565, 135, 517, 57, 41, 5, 10, 385, 120, 1512, 73, 369, 52, 4, 48, 22, 9, 344, 813, 912, 101, 12, 5, 754, 2337, 6, 754, 2880, 43, 702, 96, 792, 207, 4, 510, 735, 541, 1101, 1989, 21, 4, 175, 2873, 1600, 101, 207, 263, 1308, 1158, 4, 84, 195, 175, 29, 1512, 73, 101, 2873, 1600, 263, 217, 37, 262, 82, 691, 736, 12043, 2]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "1\n",
      "[(0, 0), (0, 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, 6), (6, 7), (7, 8), (8, 9), (9, 10), (0, 0), (0, 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, 6), (6, 7), (7, 8), (8, 9), (9, 10), (10, 11), (11, 12), (12, 13), (13, 14), (14, 15), (15, 16), (16, 17), (17, 18), (18, 19), (19, 20), (20, 21), (21, 22), (22, 23), (23, 24), (24, 25), (25, 26), (26, 27), (27, 28), (28, 29), (29, 30), (30, 31), (31, 32), (32, 33), (33, 34), (34, 35), (35, 36), (36, 37), (37, 38), (38, 39), (39, 40), (40, 41), (41, 42), (42, 43), (43, 44), (44, 45), (45, 46), (46, 47), (47, 48), (48, 49), (49, 50), (50, 51), (51, 52), (52, 53), (53, 54), (54, 55), (55, 56), (56, 57), (57, 58), (58, 59), (59, 60), (60, 61), (61, 62), (62, 63), (63, 64), (64, 65), (65, 66), (66, 67), (67, 68), (68, 69), (69, 70), (70, 71), (71, 72), (72, 73), (73, 74), (74, 75), (75, 76), (76, 77), (77, 78), (78, 79), (79, 80), (80, 81), (82, 83), (83, 84), (84, 85), (85, 86), (86, 87), (87, 88), (88, 89), (89, 90), (90, 91), (91, 92), (92, 93), (93, 94), (94, 95), (95, 96), (96, 97), (97, 98), (98, 99), (99, 100), (100, 101), (101, 102), (102, 103), (103, 104), (104, 105), (105, 106), (106, 107), (107, 108), (108, 109), (109, 110), (110, 111), (111, 112), (112, 113), (113, 114), (114, 115), (115, 116), (116, 117), (117, 118), (118, 119), (119, 120), (120, 121), (121, 122), (122, 123), (123, 124), (124, 125), (125, 126), (126, 127), (127, 128), (128, 129), (129, 130), (130, 131), (131, 132), (132, 133), (133, 134), (134, 135), (135, 136), (136, 137), (137, 138), (138, 139), (139, 140), (140, 141), (141, 142), (142, 143), (143, 144), (144, 145), (145, 146), (146, 147), (147, 148), (148, 149), (149, 150), (150, 151), (151, 152), (152, 153), (153, 154), (154, 155), (155, 156), (156, 157), (157, 158), (158, 159), (159, 160), (160, 161), (161, 162), (162, 163), (163, 164), (164, 165), (165, 166), (166, 167), (167, 168), (168, 169), (169, 170), (170, 171), (171, 172), (172, 173), (173, 174), (174, 175), (175, 176), (176, 177), (177, 178), (178, 179), (179, 180), (180, 181), (181, 182), (182, 183), (183, 184), (184, 185), (185, 186), (186, 187), (187, 188), (188, 189), (189, 190), (190, 191), (191, 193), (193, 194), (194, 196), (196, 197), (197, 198), (198, 199), (199, 200), (200, 201), (201, 202), (202, 203), (203, 204), (204, 205), (205, 206), (206, 207), (207, 208), (208, 209), (209, 210), (210, 211), (211, 212), (212, 213), (213, 214), (214, 215), (215, 217), (217, 218), (218, 220), (220, 221), (221, 222), (222, 223), (223, 224), (224, 225), (225, 226), (226, 227), (227, 228), (228, 229), (229, 230), (230, 231), (231, 232), (232, 233), (233, 234), (234, 235), (235, 236), (236, 237), (237, 238), (238, 239), (239, 241), (241, 242), (242, 243), (243, 244), (244, 245), (245, 246), (246, 247), (247, 248), (248, 249), (249, 250), (250, 251), (251, 252), (252, 253), (253, 254), (254, 255), (255, 256), (256, 257), (257, 258), (258, 259), (259, 260), (260, 264), (264, 265), (265, 266), (266, 267), (267, 268), (268, 269), (269, 270), (270, 271), (271, 272), (272, 273), (273, 274), (274, 275), (275, 276), (276, 279), (279, 280), (280, 281), (281, 282), (282, 283), (283, 284), (284, 285), (285, 286), (286, 287), (287, 288), (288, 289), (289, 290), (290, 291), (291, 292), (292, 293), (293, 294), (294, 295), (295, 296), (296, 297), (297, 298), (298, 299), (299, 300), (300, 301), (301, 302), (302, 303), (303, 304), (304, 305), (305, 306), (306, 307), (307, 308), (308, 309), (309, 310), (310, 311), (311, 312), (312, 313), (313, 314), (314, 315), (315, 316), (316, 317), (317, 318), (318, 319), (319, 320), (320, 321), (321, 322), (322, 323), (323, 324), (324, 325), (325, 326), (326, 327), (327, 328), (328, 329), (329, 330), (330, 331), (331, 332), (332, 333), (333, 334), (334, 335), (335, 336), (336, 337), (337, 338), (338, 339), (339, 340), (340, 341), (341, 342), (342, 343), (343, 344), (344, 345), (345, 346), (346, 347), (347, 348), (348, 349), (349, 350), (350, 351), (351, 352), (352, 353), (353, 354), (354, 355), (355, 356), (356, 357), (357, 358), (358, 359), (359, 360), (360, 361), (361, 362), (362, 363), (363, 364), (364, 365), (365, 366), (366, 367), (367, 368), (368, 369), (369, 370), (370, 371), (371, 372), (372, 373), (373, 374), (374, 375), (375, 376), (376, 377), (377, 378), (378, 379), (379, 380), (380, 381), (381, 382), (382, 383), (383, 384), (384, 385), (385, 386), (386, 387), (387, 388), (388, 389), (0, 0)]\n",
      "121\n",
      "122\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx in range(2):\n",
    "    print(train_ds[idx]['input_ids'])\n",
    "    print(train_ds[idx]['token_type_ids'])\n",
    "    print(train_ds[idx]['overflow_to_sample'])\n",
    "    print(train_ds[idx]['offset_mapping'])\n",
    "    print(train_ds[idx]['start_positions'])\n",
    "    print(train_ds[idx]['end_positions'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "从以上结果可以看出，数据集中的example已经被转换成了模型可以接收的feature，包括input_ids、token_type_ids、答案的起始位置等信息。\n",
    "其中：\n",
    "\n",
    "* `input_ids`: 表示输入文本的token ID。\n",
    "* `token_type_ids`: 表示对应的token属于输入的问题还是答案。（Transformer类预训练模型支持单句以及句对输入）。\n",
    "* `overflow_to_sample`: feature对应的example的编号。\n",
    "* `offset_mapping`: 每个token的起始字符和结束字符在原文中对应的index（用于生成答案文本）。\n",
    "* `start_positions`: 答案在这个feature中的开始位置。\n",
    "* `end_positions`: 答案在这个feature中的结束位置。\n",
    "\n",
    "数据处理的详细过程请参见`utils.py`。\n",
    "\n",
    "更多有关数据处理的内容，请参考[数据处理](https://paddlenlp.readthedocs.io/zh/latest/data_prepare/data_preprocess.html)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 4. Batchify和数据读入\n",
    "\n",
    "使用`paddle.io.BatchSampler`和`paddlenlp.data`中提供的方法把数据组成batch。\n",
    "\n",
    "然后使用`paddle.io.DataLoader`接口多线程异步加载数据。\n",
    "\n",
    "`batchify_fn`详解：\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/30e43d4659384375a2a2c1b890ca5a995c4324d7168e49cebf1d2a1e99161f7d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import paddle\r\n",
    "from paddlenlp.data import Stack, Dict, Pad\r\n",
    "\r\n",
    "batch_size = 12\r\n",
    "\r\n",
    "# 定义BatchSampler\r\n",
    "train_batch_sampler = paddle.io.DistributedBatchSampler(\r\n",
    "        train_ds, batch_size=batch_size, shuffle=True)\r\n",
    "\r\n",
    "dev_batch_sampler = paddle.io.BatchSampler(\r\n",
    "    dev_ds, batch_size=batch_size, shuffle=False)\r\n",
    "\r\n",
    "test_batch_sampler = paddle.io.BatchSampler(\r\n",
    "    test_ds, batch_size=batch_size, shuffle=False)\r\n",
    "\r\n",
    "# 定义batchify_fn\r\n",
    "train_batchify_fn = lambda samples, fn=Dict({\r\n",
    "    \"input_ids\": Pad(axis=0, pad_val=tokenizer.pad_token_id),\r\n",
    "    \"token_type_ids\": Pad(axis=0, pad_val=tokenizer.pad_token_type_id),\r\n",
    "    \"start_positions\": Stack(dtype=\"int64\"),\r\n",
    "    \"end_positions\": Stack(dtype=\"int64\")\r\n",
    "}): fn(samples)\r\n",
    "\r\n",
    "dev_batchify_fn = lambda samples, fn=Dict({\r\n",
    "    \"input_ids\": Pad(axis=0, pad_val=tokenizer.pad_token_id),\r\n",
    "    \"token_type_ids\": Pad(axis=0, pad_val=tokenizer.pad_token_type_id)\r\n",
    "}): fn(samples)\r\n",
    "\r\n",
    "# 构造DataLoader\r\n",
    "train_data_loader = paddle.io.DataLoader(\r\n",
    "    dataset=train_ds,\r\n",
    "    batch_sampler=train_batch_sampler,\r\n",
    "    collate_fn=train_batchify_fn,\r\n",
    "    return_list=True)\r\n",
    "\r\n",
    "dev_data_loader = paddle.io.DataLoader(\r\n",
    "    dataset=dev_ds,\r\n",
    "    batch_sampler=dev_batch_sampler,\r\n",
    "    collate_fn=dev_batchify_fn,\r\n",
    "    return_list=True)\r\n",
    "\r\n",
    "test_data_loader = paddle.io.DataLoader(\r\n",
    "    dataset=test_ds,\r\n",
    "    batch_sampler=test_batch_sampler,\r\n",
    "    collate_fn=dev_batchify_fn,\r\n",
    "    return_list=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "更多PaddleNLP内置的batchify相关API，请参考[collate](https://paddlenlp.readthedocs.io/zh/latest/source/paddlenlp.data.collate.html)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "到这里数据集准备就全部完成了，下一步我们需要组网并设计loss function。\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/fdcb44a00ede4ce08ae2652931556fb58cc903f686bf491792489353d2800e7d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 模型结构\n",
    "\n",
    "## 使用PaddleNLP一键加载预训练模型\n",
    "\n",
    "以下项目以ERNIE为例，介绍如何将预训练模型Fine-tune完成DuReader<sub>robust</sub>阅读理解任务。\n",
    "\n",
    "DuReader<sub>robust</sub>阅读理解任务的本质是答案抽取任务。根据输入的问题和文章，从预训练模型的**sequence_output**中预测答案在文章中的起始位置和结束位置。原理如下图所示：\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/bb1396fc12614dbabcfb4fcfafe9346507d4d65d0a194d75aba04b9d31bace6b)\n",
    "\n",
    "目前PaddleNLP已经内置了包括ERNIE在内的多种基于预训练模型的常用任务的下游网络，包括机器阅读理解。\n",
    "\n",
    "这些网络在`paddlenlp.transformers`下，均可实现一键调用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-06-24 09:37:06,986] [    INFO] - Downloading https://paddlenlp.bj.bcebos.com/models/transformers/ernie/ernie_v1_chn_base.pdparams and saved to /home/aistudio/.paddlenlp/models/ernie-1.0\n",
      "[2021-06-24 09:37:07,079] [    INFO] - Downloading ernie_v1_chn_base.pdparams from https://paddlenlp.bj.bcebos.com/models/transformers/ernie/ernie_v1_chn_base.pdparams\n",
      "100%|██████████| 392507/392507 [00:07<00:00, 51424.57it/s]\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1297: UserWarning: Skip loading for classifier.weight. classifier.weight is not found in the provided dict.\n",
      "  warnings.warn((\"Skip loading for {}. \".format(key) + str(err)))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1297: UserWarning: Skip loading for classifier.bias. classifier.bias is not found in the provided dict.\n",
      "  warnings.warn((\"Skip loading for {}. \".format(key) + str(err)))\n"
     ]
    }
   ],
   "source": [
    "from paddlenlp.transformers import ErnieForQuestionAnswering\r\n",
    "\r\n",
    "model = ErnieForQuestionAnswering.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 设计loss function\n",
    "\n",
    "模型的网络结构确定后我们就可以设计loss function了。\n",
    "\n",
    "ErineForQuestionAnswering模型对将ErnieModel的sequence_output拆开成start_logits和end_logits输出，所以DuReader<sub>robust</sub>的loss由start_loss和end_loss两部分组成，我们需要自己定义loss function。\n",
    "\n",
    "对于答案起始位置和结束位置的预测可以分别看成两个分类任务。所以设计的loss function如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class CrossEntropyLossForRobust(paddle.nn.Layer):\r\n",
    "    def __init__(self):\r\n",
    "        super(CrossEntropyLossForRobust, self).__init__()\r\n",
    "\r\n",
    "    def forward(self, y, label):\r\n",
    "        start_logits, end_logits = y\r\n",
    "        start_position, end_position = label\r\n",
    "        start_position = paddle.unsqueeze(start_position, axis=-1)\r\n",
    "        end_position = paddle.unsqueeze(end_position, axis=-1)\r\n",
    "        start_loss = paddle.nn.functional.cross_entropy(\r\n",
    "            input=start_logits, label=start_position)\r\n",
    "        end_loss = paddle.nn.functional.cross_entropy(\r\n",
    "            input=end_logits, label=end_position)\r\n",
    "        loss = (start_loss + end_loss) / 2\r\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "选择网络结构后，我们需要设置Fine-Tune优化策略。\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/7eca6595f338409498149cb586c077ba4933739810cf436080a2292be7e0a92d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 设置Fine-Tune优化策略\n",
    "适用于ERNIE/BERT这类Transformer模型的学习率为warmup的动态学习率。\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://ai-studio-static-online.cdn.bcebos.com/2bc624280a614a80b5449773192be460f195b13af89e4e5cbaf62bf6ac16de2c\" width=\"40%\" height=\"30%\"/> <br />\n",
    "</p>\n",
    "<br><center>图3：动态学习率示意图</center></br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 训练过程中的最大学习率\r\n",
    "learning_rate = 3e-5 \r\n",
    "\r\n",
    "# 训练轮次\r\n",
    "epochs = 2\r\n",
    "\r\n",
    "# 学习率预热比例\r\n",
    "warmup_proportion = 0.1\r\n",
    "\r\n",
    "# 权重衰减系数，类似模型正则项策略，避免模型过拟合\r\n",
    "weight_decay = 0.01\r\n",
    "\r\n",
    "num_training_steps = len(train_data_loader) * epochs\r\n",
    "\r\n",
    "# 学习率衰减策略\r\n",
    "lr_scheduler = paddlenlp.transformers.LinearDecayWithWarmup(learning_rate, num_training_steps, warmup_proportion)\r\n",
    "\r\n",
    "decay_params = [\r\n",
    "    p.name for n, p in model.named_parameters()\r\n",
    "    if not any(nd in n for nd in [\"bias\", \"norm\"])\r\n",
    "]\r\n",
    "optimizer = paddle.optimizer.AdamW(\r\n",
    "    learning_rate=lr_scheduler,\r\n",
    "    parameters=model.parameters(),\r\n",
    "    weight_decay=weight_decay,\r\n",
    "    apply_decay_param_fun=lambda x: x in decay_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "现在万事俱备，我们可以开始训练阅读理解模型啦。\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/6975542d488f4f75b385fe75d574a3aaa8e208f5e99f4acd8a8e8aea3b85c058)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 模型训练与评估\n",
    "\n",
    "\n",
    "模型训练的过程通常有以下步骤：\n",
    "\n",
    "1. 从dataloader中取出一个batch data。\n",
    "2. 将batch data喂给model，做前向计算。\n",
    "3. 将前向计算结果传给损失函数，计算loss。\n",
    "4. loss反向回传，更新梯度。重复以上步骤。\n",
    "\n",
    "每训练一个epoch时，程序通过evaluate()调用paddlenlp.metric.squad中的squad_evaluate(), compute_predictions()评估当前模型训练的效果，其中：\n",
    "\n",
    "* compute_predictions()用于生成可提交的答案；\n",
    "\n",
    "* squad_evaluate()用于返回评价指标。\n",
    "\n",
    "二者适用于所有符合squad数据格式的答案抽取任务。这类任务使用F1和exact来评估预测的答案和真实答案的相似程度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 100, epoch: 1, batch: 100, loss: 4.94566\n",
      "global step 200, epoch: 1, batch: 200, loss: 1.82053\n",
      "global step 300, epoch: 1, batch: 300, loss: 1.40963\n",
      "global step 400, epoch: 1, batch: 400, loss: 1.95402\n",
      "global step 500, epoch: 1, batch: 500, loss: 1.28218\n",
      "global step 600, epoch: 1, batch: 600, loss: 1.49382\n",
      "global step 700, epoch: 1, batch: 700, loss: 1.74916\n",
      "global step 800, epoch: 1, batch: 800, loss: 1.35063\n",
      "global step 900, epoch: 1, batch: 900, loss: 1.11943\n",
      "global step 1000, epoch: 1, batch: 1000, loss: 1.22980\n",
      "global step 1100, epoch: 1, batch: 1100, loss: 1.23139\n",
      "global step 1200, epoch: 1, batch: 1200, loss: 1.46923\n",
      "global step 1300, epoch: 1, batch: 1300, loss: 0.78345\n",
      "global step 1400, epoch: 1, batch: 1400, loss: 1.40000\n",
      "global step 1500, epoch: 2, batch: 28, loss: 0.61072\n",
      "global step 1600, epoch: 2, batch: 128, loss: 0.69888\n",
      "global step 1700, epoch: 2, batch: 228, loss: 1.00770\n",
      "global step 1800, epoch: 2, batch: 328, loss: 0.95414\n",
      "global step 1900, epoch: 2, batch: 428, loss: 0.60429\n",
      "global step 2000, epoch: 2, batch: 528, loss: 0.65532\n",
      "global step 2100, epoch: 2, batch: 628, loss: 2.14022\n",
      "global step 2200, epoch: 2, batch: 728, loss: 0.32124\n",
      "global step 2300, epoch: 2, batch: 828, loss: 0.93353\n",
      "global step 2400, epoch: 2, batch: 928, loss: 1.14577\n",
      "global step 2500, epoch: 2, batch: 1028, loss: 1.20252\n",
      "global step 2600, epoch: 2, batch: 1128, loss: 1.39452\n",
      "global step 2700, epoch: 2, batch: 1228, loss: 0.81806\n",
      "global step 2800, epoch: 2, batch: 1328, loss: 1.39922\n",
      "global step 2900, epoch: 2, batch: 1428, loss: 0.60164\n",
      "Processing example: 1000\n",
      "time per 1000: 10.139225959777832\n",
      "{\n",
      "  \"exact\": 73.25335215243472,\n",
      "  \"f1\": 86.82386195137799,\n",
      "  \"total\": 1417,\n",
      "  \"HasAns_exact\": 73.25335215243472,\n",
      "  \"HasAns_f1\": 86.82386195137799,\n",
      "  \"HasAns_total\": 1417\n",
      "}\n",
      "\n",
      "问题： 爬行垫什么材质的好\n",
      "原文： 爬行垫根据中间材料的不同可以分为:XPE爬行垫、EPE爬行垫、EVA爬行垫、PVC爬行垫;其中XPE爬行垫、EPE爬行垫都属于PE材料加保鲜膜复合而成,都是无异味的环保材料,但是XPE爬行垫是品质较好的爬行垫,韩国进口爬行垫都是这种爬行垫,而EPE爬行垫是国内厂家为了减低成本,使用EPE(珍珠棉)作为原料生产的一款爬行垫,该材料弹性差,易碎,开孔发泡防水性弱。EVA爬行垫、PVC爬行垫是用EVA或PVC作为原材料与保鲜膜复合的而成的爬行垫,或者把图案转印在原材料上,这两款爬行垫通常有异味,如果是图案转印的爬行垫,油墨外露容易脱落。 当时我儿子爬的时候,我们也买了垫子,但是始终有味。最后就没用了,铺的就的薄毯子让他爬。\n",
      "答案： XPE\n",
      "\n",
      "问题： 范冰冰多高真实身高\n",
      "原文： 真实情况是160-162。她平时谎报的168是因为不离脚穿高水台恨天高(15厘米) 图1她穿着高水台恨天高和刘亦菲一样高,(刘亦菲对外报身高172)范冰冰礼服下厚厚的高水台暴露了她的心机,对比一下两者的鞋子吧 图2 穿着高水台恨天高才和刘德华谢霆锋持平,如果她真的有168,那么加上鞋高,刘和谢都要有180?明显是不可能的。所以刘德华对外报的身高174减去10-15厘米才是范冰冰的真实身高 图3,范冰冰有一次脱鞋上场,这个最说明问题了,看看她的身体比例吧。还有目测一下她手上鞋子的鞋跟有多高多厚吧,至少超过10厘米。\n",
      "答案： 160-162\n",
      "\n",
      "问题： 小米6防水等级\n",
      "原文： 防水作为目前高端手机的标配,特别是苹果也支持防水之后,国产大多数高端旗舰手机都已经支持防水。虽然我们真的不会故意把手机放入水中,但是有了防水之后,用户心里会多一重安全感。那么近日最为火热的小米6防水吗?小米6的防水级别又是多少呢? 小编查询了很多资料发现,小米6确实是防水的,但是为了保持低调,同时为了不被别人说防水等级不够,很多资料都没有标注小米是否防水。根据评测资料显示,小米6是支持IP68级的防水,是绝对能够满足日常生活中的防水需求的。\n",
      "答案： IP68级\n",
      "\n",
      "问题： 怀孕多久会有反应\n",
      "原文： 这位朋友你好,女性出现妊娠反应一般是从6-12周左右,也就是女性怀孕1个多月就会开始出现反应,第3个月的时候,妊辰反应基本结束。 而大部分女性怀孕初期都会出现恶心、呕吐的感觉,这些症状都是因人而异的,除非恶心、呕吐的非常厉害,才需要就医,否则这些都是刚怀孕的的正常症状。1-3个月的时候可以观察一下自己的皮肤,一般女性怀孕初期可能会产生皮肤色素沉淀或是腹壁产生妊娠纹,特别是在怀孕的后期更加明显。 还有很多女性怀孕初期会出现疲倦、嗜睡的情况。怀孕三个月的时候,膀胱会受到日益胀大的子宫的压迫,容量会变小,所以怀孕期间也会有尿频的现象出现。月经停止也是刚怀孕最容易出现的症状,只要是平时月经正常的女性,在性行为后超过正常经期两周,就有可能是怀孕了。 如果你想判断自己是否怀孕,可以看看自己有没有这些反应。当然这也只是多数人的怀孕表现,也有部分女性怀孕表现并不完全是这样,如果你无法确定自己是否怀孕,最好去医院检查一下。\n",
      "答案： 1个多月\n",
      "\n",
      "问题： 研发费用加计扣除比例\n",
      "原文： 【东奥会计在线——中级会计职称频道推荐】根据《关于提高科技型中小企业研究开发费用税前加计扣除比例的通知》的规定,研发费加计扣除比例提高到75%。|财政部、国家税务总局、科技部发布《关于提高科技型中小企业研究开发费用税前加计扣除比例的通知》。|通知称,为进一步激励中小企业加大研发投入,支持科技创新,就提高科技型中小企业研究开发费用(以下简称研发费用)税前加计扣除比例有关问题发布通知。|通知明确,科技型中小企业开展研发活动中实际发生的研发费用,未形成无形资产计入当期损益的,在按规定据实扣除的基础上,在2017年1月1日至2019年12月31日期间,再按照实际发生额的75%在税前加计扣除;形成无形资产的,在上述期间按照无形资产成本的175%在税前摊销。|科技型中小企业享受研发费用税前加计扣除政策的其他政策口径按照《财政部国家税务总局科技部关于完善研究开发费用税前加计扣除政策的通知》(财税〔2015〕119号)规定执行。|科技型中小企业条件和管理办法由科技部、财政部和国家税务总局另行发布。科技、财政和税务部门应建立信息共享机制,及时共享科技型中小企业的相关信息,加强协调配合,保障优惠政策落实到位。|上一篇文章:关于2016年度企业研究开发费用税前加计扣除政策企业所得税纳税申报问题的公告 下一篇文章:关于提高科技型中小企业研究开发费用税前加计扣除比例的通知\n",
      "答案： 75%\n"
     ]
    }
   ],
   "source": [
    "from utils import evaluate\r\n",
    "\r\n",
    "criterion = CrossEntropyLossForRobust()\r\n",
    "global_step = 0\r\n",
    "for epoch in range(1, epochs + 1):\r\n",
    "    for step, batch in enumerate(train_data_loader, start=1):\r\n",
    "\r\n",
    "        global_step += 1\r\n",
    "        input_ids, segment_ids, start_positions, end_positions = batch\r\n",
    "        logits = model(input_ids=input_ids, token_type_ids=segment_ids)\r\n",
    "        loss = criterion(logits, (start_positions, end_positions))\r\n",
    "\r\n",
    "        if global_step % 100 == 0 :\r\n",
    "            print(\"global step %d, epoch: %d, batch: %d, loss: %.5f\" % (global_step, epoch, step, loss))\r\n",
    "\r\n",
    "        loss.backward()\r\n",
    "        optimizer.step()\r\n",
    "        lr_scheduler.step()\r\n",
    "        optimizer.clear_grad()\r\n",
    "\r\n",
    "evaluate(model=model, data_loader=dev_data_loader) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: 1000\n",
      "time per 1000: 10.01546597480774\n",
      "Processing example: 2000\n",
      "time per 1000: 10.139619827270508\n",
      "Processing example: 3000\n",
      "time per 1000: 9.975267171859741\n",
      "Processing example: 4000\n",
      "time per 1000: 10.106710195541382\n",
      "Processing example: 5000\n",
      "time per 1000: 9.516096353530884\n",
      "Processing example: 6000\n",
      "time per 1000: 9.609010934829712\n",
      "Processing example: 7000\n",
      "time per 1000: 9.462340831756592\n",
      "Processing example: 8000\n",
      "time per 1000: 9.752124071121216\n",
      "Processing example: 9000\n",
      "time per 1000: 10.262354612350464\n",
      "Processing example: 10000\n",
      "time per 1000: 10.147829294204712\n",
      "Processing example: 11000\n",
      "time per 1000: 9.983196020126343\n",
      "Processing example: 12000\n",
      "time per 1000: 10.260183095932007\n",
      "Processing example: 13000\n",
      "time per 1000: 9.999037504196167\n",
      "Processing example: 14000\n",
      "time per 1000: 9.702670335769653\n",
      "Processing example: 15000\n",
      "time per 1000: 10.171399116516113\n",
      "Processing example: 16000\n",
      "time per 1000: 9.482449293136597\n",
      "Processing example: 17000\n",
      "time per 1000: 9.500334978103638\n",
      "Processing example: 18000\n",
      "time per 1000: 9.8500235080719\n",
      "Processing example: 19000\n",
      "time per 1000: 9.67605710029602\n",
      "Processing example: 20000\n",
      "time per 1000: 9.784584999084473\n",
      "Processing example: 21000\n",
      "time per 1000: 10.31256890296936\n",
      "Processing example: 22000\n",
      "time per 1000: 10.255650043487549\n",
      "Processing example: 23000\n",
      "time per 1000: 10.09070634841919\n",
      "Processing example: 24000\n",
      "time per 1000: 9.890270709991455\n",
      "Processing example: 25000\n",
      "time per 1000: 9.873517513275146\n",
      "Processing example: 26000\n",
      "time per 1000: 9.541719198226929\n",
      "Processing example: 27000\n",
      "time per 1000: 10.26811695098877\n",
      "Processing example: 28000\n",
      "time per 1000: 10.100624322891235\n",
      "Processing example: 29000\n",
      "time per 1000: 10.150312662124634\n",
      "Processing example: 30000\n",
      "time per 1000: 9.77371096611023\n",
      "Processing example: 31000\n",
      "time per 1000: 9.666250228881836\n",
      "Processing example: 32000\n",
      "time per 1000: 9.755880355834961\n",
      "Processing example: 33000\n",
      "time per 1000: 9.571914196014404\n",
      "Processing example: 34000\n",
      "time per 1000: 10.114226579666138\n",
      "Processing example: 35000\n",
      "time per 1000: 9.55317735671997\n",
      "Processing example: 36000\n",
      "time per 1000: 10.142505645751953\n",
      "Processing example: 37000\n",
      "time per 1000: 9.405508279800415\n",
      "Processing example: 38000\n",
      "time per 1000: 9.946635961532593\n",
      "Processing example: 39000\n",
      "time per 1000: 9.878861904144287\n",
      "Processing example: 40000\n",
      "time per 1000: 10.024235725402832\n",
      "Processing example: 41000\n",
      "time per 1000: 10.45734977722168\n",
      "Processing example: 42000\n",
      "time per 1000: 10.055162191390991\n",
      "Processing example: 43000\n",
      "time per 1000: 10.224614143371582\n",
      "Processing example: 44000\n",
      "time per 1000: 9.751557111740112\n",
      "Processing example: 45000\n",
      "time per 1000: 9.594443082809448\n",
      "Processing example: 46000\n",
      "time per 1000: 9.867080450057983\n",
      "Processing example: 47000\n",
      "time per 1000: 9.537430763244629\n",
      "Processing example: 48000\n",
      "time per 1000: 10.173825979232788\n",
      "Processing example: 49000\n",
      "time per 1000: 9.605824708938599\n",
      "Processing example: 50000\n",
      "time per 1000: 9.448263168334961\n",
      "Processing example: 51000\n",
      "time per 1000: 9.921022176742554\n",
      "Processing example: 52000\n",
      "time per 1000: 9.646288394927979\n",
      "Processing example: 53000\n",
      "time per 1000: 9.556435823440552\n",
      "Processing example: 54000\n",
      "time per 1000: 9.903976440429688\n",
      "Processing example: 55000\n",
      "time per 1000: 9.79933762550354\n",
      "Processing example: 56000\n",
      "time per 1000: 10.181400775909424\n",
      "Processing example: 57000\n",
      "time per 1000: 9.96089792251587\n",
      "Processing example: 58000\n",
      "time per 1000: 10.192773580551147\n",
      "Processing example: 59000\n",
      "time per 1000: 9.761321544647217\n",
      "Processing example: 60000\n",
      "time per 1000: 9.817643880844116\n",
      "Processing example: 61000\n",
      "time per 1000: 9.84264874458313\n",
      "Processing example: 62000\n",
      "time per 1000: 10.080573320388794\n",
      "Processing example: 63000\n",
      "time per 1000: 9.683675050735474\n",
      "Processing example: 64000\n",
      "time per 1000: 9.880760192871094\n",
      "Processing example: 65000\n",
      "time per 1000: 9.882460117340088\n",
      "Processing example: 66000\n",
      "time per 1000: 9.89532470703125\n",
      "Processing example: 67000\n",
      "time per 1000: 9.55311131477356\n",
      "Processing example: 68000\n",
      "time per 1000: 9.527262926101685\n",
      "Processing example: 69000\n",
      "time per 1000: 10.048983573913574\n",
      "\n",
      "问题： 220v一安等于多少瓦\n",
      "原文： 在220交流电的状态下一安等于220瓦.基于32太1.5匹用多大的开关计算方法是：1匹=0.735瓦. 0.735*1.5*32=35.28千瓦  1千瓦=4.5安  35.28*4.5=158.26安  这儿是实际的电流,在现实应用过程中不能用160安的  开关,单个1.5匹启动时有一个较大的启动电流,在实际用用是乘以105倍：158.26*1.5=237.39安. 开关的电流应该是250A的空气开关.\n",
      "答案： 220瓦\n",
      "\n",
      "问题： 氧化铜和稀盐酸的离子方程式\n",
      "原文： 化学方程式：CuO+2HCl=CuCl2+H2O 书写离子方程式时,只有强电解质（强酸、强碱、盐）拆开写成离子形式. 离子方程式： CuO+2H+=Cu^2+ +H2O\n",
      "答案： CuO+2HCl=CuCl2+H2O\n",
      "\n",
      "问题： 刀塔传奇98元英雄排行\n",
      "原文： 让我们把目光放到刀塔传奇中去,看看那些在竞技场的刀山火海中异军突起的英雄,很多只是得益于一个小小的改动,却改变了竞技场的整个格局。http://www.18183.com/dtcq/syzs/wanjiayc/164981.html|1艾吉奥梦境打架都可以2科学怪人梦境团本高分必备3魔像可以打吸血鬼一个梦境4白银很看好她期待她的觉醒5凹凸曼打猴子,其他可有可无\n",
      "答案： 164981.html\n",
      "\n",
      "问题： 新款汉兰达什么时候上市\n",
      "原文： 新一代丰田汉兰达虽然早在2013年的纽约车展上已经正式亮相，但依然迟迟未见在国内出现。据目前获得的最新消息称，现款汉兰达将于2015年1月正式停产，新一代车型预计将于2015年一季度末投产，并于2015年上半年正式亮相。目前广汽丰田新一代汉兰达申报图已经在网上曝光。\n",
      "答案： 2015年上半年\n",
      "\n",
      "问题： 一立方分米等于多少立方米\n",
      "原文： 正方体用米作单位计算：  1*1*1=1立方米 把米化成分米做单位就是10分米,用分米作单位计算：  10*10*10=1000立方分米 所以,1立方米=1000立方分米\n",
      "答案： 0=1000立方\n"
     ]
    }
   ],
   "source": [
    "# 传入test_data_loader，并将is_test参数设为True，即可生成千言比赛可提交的结果。\r\n",
    "evaluate(model=model, data_loader=test_data_loader, is_test=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "以上基线实现基于PaddleNLP，开源不易，希望大家多多支持~ \n",
    "**记得给[PaddleNLP](https://github.com/PaddlePaddle/PaddleNLP)点个小小的Star⭐**\n",
    "\n",
    "GitHub地址：[https://github.com/PaddlePaddle/PaddleNLP](https://github.com/PaddlePaddle/PaddleNLP)\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/a0e8ca7743ea4fe9aa741682a63e767f8c48dc55981f4e44a40e0e00d3ab369e)\n",
    "\n",
    "**更多使用方法可参考PaddleNLP教程**\n",
    "\n",
    "- [使用seq2vec模块进行句子情感分类](https://aistudio.baidu.com/aistudio/projectdetail/1283423)\n",
    "- [使用预训练模型ERNIE优化情感分析](https://aistudio.baidu.com/aistudio/projectdetail/1294333)\n",
    "- [使用BiGRU-CRF模型完成快递单信息抽取](https://aistudio.baidu.com/aistudio/projectdetail/1317771)\n",
    "- [使用预训练模型ERNIE优化快递单信息抽取](https://aistudio.baidu.com/aistudio/projectdetail/1329361)\n",
    "- [使用Seq2Seq模型完成自动对联](https://aistudio.baidu.com/aistudio/projectdetail/1321118)\n",
    "- [使用预训练模型ERNIE-GEN自动写诗](https://aistudio.baidu.com/aistudio/projectdetail/1339888)\n",
    "- [使用TCN网络完成新冠疫情病例数预测](https://aistudio.baidu.com/aistudio/projectdetail/1290873)\n",
    "- [自定义数据集实现文本多分类任务](https://aistudio.baidu.com/aistudio/projectdetail/1468469)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 加入交流群，一起学习吧\n",
    "\n",
    "现在就加入PaddleNLP的QQ技术交流群，一起交流NLP技术吧！\n",
    "\n",
    "<img src=\"https://ai-studio-static-online.cdn.bcebos.com/d953727af0c24a7c806ab529495f0904f22f809961be420b8c88cdf59b837394\" width=\"200\" height=\"250\" >"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.1.0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
