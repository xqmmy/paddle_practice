{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 作业\n",
    "\n",
    "更换数据集MSRA和ERNIE-Gram或BERT等预训练模型。\n",
    "\n",
    "- 数据集：\n",
    "`train_ds, test_ds = load_dataset(\"msra_ner\", splits=[\"train\", \"test\"])`\n",
    "- 模型：\n",
    "\t将`from paddlenlp.transformers import ErnieTokenizer, ErnieForTokenClassification`换成相应的模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 使用PaddleNLP语义预训练模型ERNIE完成快递单信息抽取\n",
    "\n",
    "\n",
    "**注意**\n",
    "\n",
    "本项目代码需要使用GPU环境来运行:\n",
    "\n",
    "<img src=\"https://ai-studio-static-online.cdn.bcebos.com/767f625548714f03b105b6ccb3aa78df9080e38d329e445380f505ddec6c7042\" width=\"40%\" height=\"40%\">\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "命名实体识别是NLP中一项非常基础的任务，是信息提取、问答系统、句法分析、机器翻译等众多NLP任务的重要基础工具。命名实体识别的准确度，决定了下游任务的效果，是NLP中的一个基础问题。在NER任务提供了两种解决方案，一类LSTM/GRU + CRF，通过RNN类的模型来抽取底层文本的信息，而CRF(条件随机场)模型来学习底层Token之间的联系；另外一类是通过预训练模型，例如ERNIE，BERT模型，直接来预测Token的标签信息。\n",
    "\n",
    "本项目将演示如何使用PaddleNLP语义预训练模型ERNIE完成从快递单中抽取姓名、电话、省、市、区、详细地址等内容，形成结构化信息。辅助物流行业从业者进行有效信息的提取，从而降低客户填单的成本。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "在2017年之前，工业界和学术界对文本处理依赖于序列模型[Recurrent Neural Network (RNN)](https://baike.baidu.com/item/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/23199490?fromtitle=RNN&fromid=5707183&fr=aladdin).\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"http://colah.github.io/posts/2015-09-NN-Types-FP/img/RNN-general.png\" width=\"40%\" height=\"30%\"> <br />\n",
    "</p><br><center>图1：RNN示意图</center></br>\n",
    "\n",
    "[基于BiGRU+CRF的快递单信息抽取](https://aistudio.baidu.com/aistudio/projectdetail/1317771)项目介绍了如何使用序列模型完成快递单信息抽取任务。\n",
    "<br>\n",
    "\n",
    "近年来随着深度学习的发展，模型参数的数量飞速增长。为了训练这些参数，需要更大的数据集来避免过拟合。然而，对于大部分NLP任务来说，构建大规模的标注数据集非常困难（成本过高），特别是对于句法和语义相关的任务。相比之下，大规模的未标注语料库的构建则相对容易。为了利用这些数据，我们可以先从其中学习到一个好的表示，再将这些表示应用到其他任务中。最近的研究表明，基于大规模未标注语料库的预训练模型（Pretrained Models, PTM) 在NLP任务上取得了很好的表现。\n",
    "\n",
    "近年来，大量的研究表明基于大型语料库的预训练模型（Pretrained Models, PTM）可以学习通用的语言表示，有利于下游NLP任务，同时能够避免从零开始训练模型。随着计算能力的不断提高，深度模型的出现（即 Transformer）和训练技巧的增强使得 PTM 不断发展，由浅变深。\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://ai-studio-static-online.cdn.bcebos.com/327f44ff3ed24493adca5ddc4dc24bf61eebe67c84a6492f872406f464fde91e\" width=\"60%\" height=\"50%\"> <br />\n",
    "</p><br><center>图2：预训练模型一览，图片来源于：https://github.com/thunlp/PLMpapers</center></br>\n",
    "                                                                                                                             \n",
    "本示例展示了以ERNIE([Enhanced Representation through Knowledge Integration](https://arxiv.org/pdf/1904.09223))为代表的预训练模型如何Finetune完成序列标注任务。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**记得给[PaddleNLP](https://github.com/PaddlePaddle/PaddleNLP)点个小小的Star⭐**\n",
    "\n",
    "开源不易，希望大家多多支持~ \n",
    "\n",
    "GitHub地址：[https://github.com/PaddlePaddle/PaddleNLP](https://github.com/PaddlePaddle/PaddleNLP)\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/a0e8ca7743ea4fe9aa741682a63e767f8c48dc55981f4e44a40e0e00d3ab369e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "AI Studio平台后续会默认安装PaddleNLP，在此之前可使用如下命令安装。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirror.baidu.com/pypi/simple/\n",
      "Collecting paddlenlp\n",
      "\u001b[?25l  Downloading https://mirror.baidu.com/pypi/packages/b1/e9/128dfc1371db3fc2fa883d8ef27ab6b21e3876e76750a43f58cf3c24e707/paddlenlp-2.0.2-py3-none-any.whl (426kB)\n",
      "\u001b[K     |████████████████████████████████| 430kB 14.8MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: h5py in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (2.9.0)\n",
      "Requirement already satisfied, skipping upgrade: colorlog in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (4.1.0)\n",
      "Requirement already satisfied, skipping upgrade: visualdl in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (2.1.1)\n",
      "Requirement already satisfied, skipping upgrade: colorama in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (0.4.4)\n",
      "Requirement already satisfied, skipping upgrade: jieba in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (0.42.1)\n",
      "Requirement already satisfied, skipping upgrade: multiprocess in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (0.70.11.1)\n",
      "Requirement already satisfied, skipping upgrade: seqeval in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (1.2.2)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.7 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from h5py->paddlenlp) (1.20.3)\n",
      "Requirement already satisfied, skipping upgrade: six in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from h5py->paddlenlp) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: pre-commit in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (1.21.0)\n",
      "Requirement already satisfied, skipping upgrade: flask>=1.1.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (1.1.1)\n",
      "Requirement already satisfied, skipping upgrade: shellcheck-py in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (0.7.1.1)\n",
      "Requirement already satisfied, skipping upgrade: requests in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (2.22.0)\n",
      "Requirement already satisfied, skipping upgrade: Flask-Babel>=1.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (1.0.0)\n",
      "Requirement already satisfied, skipping upgrade: flake8>=3.7.9 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (3.8.2)\n",
      "Requirement already satisfied, skipping upgrade: bce-python-sdk in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (0.8.53)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.11.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (3.14.0)\n",
      "Requirement already satisfied, skipping upgrade: Pillow>=7.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (7.1.2)\n",
      "Requirement already satisfied, skipping upgrade: dill>=0.3.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from multiprocess->paddlenlp) (0.3.3)\n",
      "Requirement already satisfied, skipping upgrade: scikit-learn>=0.21.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from seqeval->paddlenlp) (0.24.2)\n",
      "Requirement already satisfied, skipping upgrade: toml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (0.10.0)\n",
      "Requirement already satisfied, skipping upgrade: aspy.yaml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (1.3.0)\n",
      "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (0.23)\n",
      "Requirement already satisfied, skipping upgrade: cfgv>=2.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (2.0.1)\n",
      "Requirement already satisfied, skipping upgrade: nodeenv>=0.11.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (1.3.4)\n",
      "Requirement already satisfied, skipping upgrade: pyyaml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (5.1.2)\n",
      "Requirement already satisfied, skipping upgrade: virtualenv>=15.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (16.7.9)\n",
      "Requirement already satisfied, skipping upgrade: identify>=1.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (1.4.10)\n",
      "Requirement already satisfied, skipping upgrade: itsdangerous>=0.24 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl->paddlenlp) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: click>=5.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl->paddlenlp) (7.0)\n",
      "Requirement already satisfied, skipping upgrade: Werkzeug>=0.15 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl->paddlenlp) (0.16.0)\n",
      "Requirement already satisfied, skipping upgrade: Jinja2>=2.10.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl->paddlenlp) (2.10.1)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl->paddlenlp) (1.25.6)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl->paddlenlp) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl->paddlenlp) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl->paddlenlp) (2019.9.11)\n",
      "Requirement already satisfied, skipping upgrade: Babel>=2.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Flask-Babel>=1.0.0->visualdl->paddlenlp) (2.8.0)\n",
      "Requirement already satisfied, skipping upgrade: pytz in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Flask-Babel>=1.0.0->visualdl->paddlenlp) (2019.3)\n",
      "Requirement already satisfied, skipping upgrade: pyflakes<2.3.0,>=2.2.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl->paddlenlp) (2.2.0)\n",
      "Requirement already satisfied, skipping upgrade: mccabe<0.7.0,>=0.6.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl->paddlenlp) (0.6.1)\n",
      "Requirement already satisfied, skipping upgrade: pycodestyle<2.7.0,>=2.6.0a1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl->paddlenlp) (2.6.0)\n",
      "Requirement already satisfied, skipping upgrade: pycryptodome>=3.8.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from bce-python-sdk->visualdl->paddlenlp) (3.9.9)\n",
      "Requirement already satisfied, skipping upgrade: future>=0.6.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from bce-python-sdk->visualdl->paddlenlp) (0.18.0)\n",
      "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp) (0.14.1)\n",
      "Requirement already satisfied, skipping upgrade: threadpoolctl>=2.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp) (2.1.0)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.19.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp) (1.6.3)\n",
      "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->pre-commit->visualdl->paddlenlp) (0.6.0)\n",
      "Requirement already satisfied, skipping upgrade: MarkupSafe>=0.23 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Jinja2>=2.10.1->flask>=1.1.1->visualdl->paddlenlp) (1.1.1)\n",
      "Requirement already satisfied, skipping upgrade: more-itertools in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from zipp>=0.5->importlib-metadata; python_version < \"3.8\"->pre-commit->visualdl->paddlenlp) (7.2.0)\n",
      "Installing collected packages: paddlenlp\n",
      "  Found existing installation: paddlenlp 2.0.1\n",
      "    Uninstalling paddlenlp-2.0.1:\n",
      "      Successfully uninstalled paddlenlp-2.0.1\n",
      "Successfully installed paddlenlp-2.0.2\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade paddlenlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中\u0002共\u0002中\u0002央\u0002致\u0002中\u0002国\u0002致\u0002公\u0002党\u0002十\u0002一\u0002大\u0002的\u0002贺\u0002词\u0002各\u0002位\u0002代\u0002表\u0002、\u0002各\u0002位\u0002同\u0002志\u0002：\u0002在\u0002中\u0002国\u0002致\u0002公\u0002党\u0002第\u0002十\u0002一\u0002次\u0002全\u0002国\u0002代\u0002表\u0002大\u0002会\u0002隆\u0002重\u0002召\u0002开\u0002之\u0002际\u0002，\u0002中\u0002国\u0002共\u0002产\u0002党\u0002中\u0002央\u0002委\u0002员\u0002会\u0002谨\u0002向\u0002大\u0002会\u0002表\u0002示\u0002热\u0002烈\u0002的\u0002祝\u0002贺\u0002，\u0002向\u0002致\u0002公\u0002党\u0002的\u0002同\u0002志\u0002们\u0002致\u0002以\u0002亲\u0002切\u0002的\u0002问\u0002候\u0002！\tB-ORG\u0002I-ORG\u0002I-ORG\u0002I-ORG\u0002O\u0002B-ORG\u0002I-ORG\u0002I-ORG\u0002I-ORG\u0002I-ORG\u0002I-ORG\u0002I-ORG\u0002I-ORG\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002B-ORG\u0002I-ORG\u0002I-ORG\u0002I-ORG\u0002I-ORG\u0002I-ORG\u0002I-ORG\u0002I-ORG\u0002I-ORG\u0002I-ORG\u0002I-ORG\u0002I-ORG\u0002I-ORG\u0002I-ORG\u0002I-ORG\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002B-ORG\u0002I-ORG\u0002I-ORG\u0002I-ORG\u0002I-ORG\u0002I-ORG\u0002I-ORG\u0002I-ORG\u0002I-ORG\u0002I-ORG\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002B-ORG\u0002I-ORG\u0002I-ORG\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\r\n",
      "在\u0002过\u0002去\u0002的\u0002五\u0002年\u0002中\u0002，\u0002致\u0002公\u0002党\u0002在\u0002邓\u0002小\u0002平\u0002理\u0002论\u0002指\u0002引\u0002下\u0002，\u0002遵\u0002循\u0002社\u0002会\u0002主\u0002义\u0002初\u0002级\u0002阶\u0002段\u0002的\u0002基\u0002本\u0002路\u0002线\u0002，\u0002努\u0002力\u0002实\u0002践\u0002致\u0002公\u0002党\u0002十\u0002大\u0002提\u0002出\u0002的\u0002发\u0002挥\u0002参\u0002政\u0002党\u0002职\u0002能\u0002、\u0002加\u0002强\u0002自\u0002身\u0002建\u0002设\u0002的\u0002基\u0002本\u0002任\u0002务\u0002。\tO\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002B-ORG\u0002I-ORG\u0002I-ORG\u0002O\u0002B-PER\u0002I-PER\u0002I-PER\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002B-ORG\u0002I-ORG\u0002I-ORG\u0002I-ORG\u0002I-ORG\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\r\n",
      "高\u0002举\u0002爱\u0002国\u0002主\u0002义\u0002和\u0002社\u0002会\u0002主\u0002义\u0002两\u0002面\u0002旗\u0002帜\u0002，\u0002团\u0002结\u0002全\u0002体\u0002成\u0002员\u0002以\u0002及\u0002所\u0002联\u0002系\u0002的\u0002归\u0002侨\u0002、\u0002侨\u0002眷\u0002，\u0002发\u0002扬\u0002爱\u0002国\u0002革\u0002命\u0002的\u0002光\u0002荣\u0002传\u0002统\u0002，\u0002为\u0002统\u0002一\u0002祖\u0002国\u0002、\u0002振\u0002兴\u0002中\u0002华\u0002而\u0002努\u0002力\u0002奋\u0002斗\u0002；\u0002紧\u0002紧\u0002围\u0002绕\u0002国\u0002家\u0002的\u0002中\u0002心\u0002工\u0002作\u0002，\u0002联\u0002系\u0002改\u0002革\u0002和\u0002建\u0002设\u0002中\u0002的\u0002重\u0002大\u0002问\u0002题\u0002以\u0002及\u0002人\u0002民\u0002群\u0002众\u0002普\u0002遍\u0002关\u0002心\u0002的\u0002社\u0002会\u0002问\u0002题\u0002，\u0002深\u0002入\u0002开\u0002展\u0002调\u0002查\u0002研\u0002究\u0002，\u0002就\u0002经\u0002济\u0002建\u0002设\u0002、\u0002侨\u0002务\u0002政\u0002策\u0002、\u0002文\u0002教\u0002卫\u0002生\u0002、\u0002对\u0002外\u0002开\u0002放\u0002、\u0002精\u0002神\u0002文\u0002明\u0002建\u0002设\u0002等\u0002问\u0002题\u0002，\u0002提\u0002出\u0002了\u0002许\u0002多\u0002宝\u0002贵\u0002的\u0002意\u0002见\u0002和\u0002建\u0002议\u0002，\u0002受\u0002到\u0002有\u0002关\u0002方\u0002面\u0002高\u0002度\u0002重\u0002视\u0002；\u0002致\u0002公\u0002党\u0002中\u0002央\u0002领\u0002导\u0002人\u0002多\u0002次\u0002参\u0002加\u0002中\u0002共\u0002中\u0002央\u0002和\u0002国\u0002务\u0002院\u0002举\u0002行\u0002的\u0002民\u0002主\u0002党\u0002派\u0002人\u0002士\u0002座\u0002谈\u0002会\u0002、\u0002协\u0002商\u0002会\u0002，\u0002参\u0002与\u0002国\u0002家\u0002大\u0002政\u0002方\u0002针\u0002的\u0002协\u0002商\u0002，\u0002认\u0002真\u0002履\u0002行\u0002参\u0002政\u0002议\u0002政\u0002、\u0002民\u0002主\u0002监\u0002督\u0002职\u0002能\u0002；\u0002广\u0002大\u0002成\u0002员\u0002在\u0002做\u0002好\u0002本\u0002职\u0002工\u0002作\u0002的\u0002同\u0002时\u0002，\u0002把\u0002科\u0002技\u0002扶\u0002贫\u0002、\u0002智\u0002力\u0002支\u0002边\u0002作\u0002为\u0002为\u0002社\u0002会\u0002主\u0002义\u0002建\u0002设\u0002服\u0002务\u0002的\u0002一\u0002项\u0002重\u0002要\u0002工\u0002作\u0002，\u0002不\u0002断\u0002开\u0002拓\u0002进\u0002取\u0002，\u0002取\u0002得\u0002了\u0002可\u0002喜\u0002的\u0002成\u0002绩\u0002，\u0002为\u0002促\u0002进\u0002社\u0002会\u0002主\u0002义\u0002物\u0002质\u0002文\u0002明\u0002和\u0002精\u0002神\u0002文\u0002明\u0002建\u0002设\u0002作\u0002出\u0002了\u0002积\u0002极\u0002贡\u0002献\u0002；\u0002结\u0002合\u0002自\u0002身\u0002的\u0002特\u0002点\u0002，\u0002充\u0002分\u0002发\u0002挥\u0002与\u0002海\u0002外\u0002联\u0002系\u0002广\u0002泛\u0002的\u0002优\u0002势\u0002，\u0002积\u0002极\u0002开\u0002展\u0002海\u0002外\u0002联\u0002络\u0002工\u0002作\u0002，\u0002为\u0002促\u0002进\u0002祖\u0002国\u0002的\u0002和\u0002平\u0002统\u0002一\u0002作\u0002出\u0002了\u0002不\u0002懈\u0002的\u0002努\u0002力\u0002。\tO\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002B-LOC\u0002I-LOC\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002B-ORG\u0002I-ORG\u0002I-ORG\u0002I-ORG\u0002I-ORG\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002B-ORG\u0002I-ORG\u0002I-ORG\u0002I-ORG\u0002O\u0002B-ORG\u0002I-ORG\u0002I-ORG\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\r\n",
      "在\u0002此\u0002，\u0002中\u0002共\u0002中\u0002央\u0002谨\u0002向\u0002致\u0002公\u0002党\u0002中\u0002央\u0002以\u0002及\u0002全\u0002体\u0002成\u0002员\u0002致\u0002以\u0002崇\u0002高\u0002的\u0002敬\u0002意\u0002！\tO\u0002O\u0002O\u0002B-ORG\u0002I-ORG\u0002I-ORG\u0002I-ORG\u0002O\u0002O\u0002B-ORG\u0002I-ORG\u0002I-ORG\u0002I-ORG\u0002I-ORG\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\r\n",
      "不\u0002久\u0002前\u0002，\u0002中\u0002国\u0002共\u0002产\u0002党\u0002召\u0002开\u0002了\u0002举\u0002世\u0002瞩\u0002目\u0002的\u0002第\u0002十\u0002五\u0002次\u0002全\u0002国\u0002代\u0002表\u0002大\u0002会\u0002。\tO\u0002O\u0002O\u0002O\u0002B-ORG\u0002I-ORG\u0002I-ORG\u0002I-ORG\u0002I-ORG\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002B-ORG\u0002I-ORG\u0002I-ORG\u0002I-ORG\u0002I-ORG\u0002I-ORG\u0002I-ORG\u0002I-ORG\u0002I-ORG\u0002I-ORG\u0002O\r\n"
     ]
    }
   ],
   "source": [
    "# 下载并解压数据集\n",
    "# from paddle.utils.download import get_path_from_url\n",
    "# # URL = \"https://paddlenlp.bj.bcebos.com/paddlenlp/datasets/msra_ner.tar.gz\"\n",
    "# URL = 'https://paddlenlp.bj.bcebos.com/datasets/msra_ner.tar.gz'\n",
    "# get_path_from_url(URL, \"./\")\n",
    "\n",
    "# # 查看预测的数据\n",
    "# !head -n 5 msra_ner/test.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import time\n",
    "import paddle\n",
    "from paddlenlp.datasets import MapDataset\n",
    "from paddlenlp.data import Stack, Tuple, Pad, Dict\n",
    "# from paddlenlp.transformers import ErnieTokenizer, ErnieForTokenClassification\n",
    "from paddlenlp.transformers import BertForTokenClassification, BertTokenizer\n",
    "from paddlenlp.metrics import ChunkEvaluator\n",
    "from utils import convert_example, evaluate, predict, load_dict\n",
    "from paddlenlp.datasets import load_dataset\n",
    "from paddle.io import DataLoader\n",
    "from paddlenlp.transformers import LinearDecayWithWarmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_ds, test_ds = load_dataset(\r\n",
    "        'msra_ner', splits=('train', 'test'), lazy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-06-16 13:27:39,673] [    INFO] - Found /home/aistudio/.paddlenlp/models/bert-wwm-chinese/bert-wwm-chinese-vocab.txt\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-wwm-chinese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluate(model, loss_fct, metric, data_loader, label_num):\r\n",
    "    model.eval()\r\n",
    "    metric.reset()\r\n",
    "    avg_loss, precision, recall, f1_score = 0, 0, 0, 0\r\n",
    "    for batch in data_loader:\r\n",
    "        input_ids, token_type_ids, length, labels = batch\r\n",
    "        logits = model(input_ids, token_type_ids)\r\n",
    "        loss = loss_fct(logits, labels)\r\n",
    "        avg_loss = paddle.mean(loss)\r\n",
    "        preds = logits.argmax(axis=2)\r\n",
    "        num_infer_chunks, num_label_chunks, num_correct_chunks = metric.compute(\r\n",
    "            length, preds, labels)\r\n",
    "        metric.update(num_infer_chunks.numpy(),\r\n",
    "                      num_label_chunks.numpy(), num_correct_chunks.numpy())\r\n",
    "        precision, recall, f1_score = metric.accumulate()\r\n",
    "    print(\"eval loss: %f, precision: %f, recall: %f, f1: %f\" %\r\n",
    "          (avg_loss, precision, recall, f1_score))\r\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(example, tokenizer, no_entity_id,\r\n",
    "                              max_seq_len=512):\r\n",
    "    labels = example['labels']\r\n",
    "    example = example['tokens']\r\n",
    "    tokenized_input = tokenizer(\r\n",
    "        example,\r\n",
    "        return_length=True,\r\n",
    "        is_split_into_words=True,\r\n",
    "        max_seq_len=max_seq_len)\r\n",
    "\r\n",
    "    # -2 for [CLS] and [SEP]\r\n",
    "    if len(tokenized_input['input_ids']) - 2 < len(labels):\r\n",
    "        labels = labels[:len(tokenized_input['input_ids']) - 2]\r\n",
    "    tokenized_input['labels'] = [no_entity_id] + labels + [no_entity_id]\r\n",
    "    tokenized_input['labels'] += [no_entity_id] * (\r\n",
    "        len(tokenized_input['input_ids']) - len(tokenized_input['labels']))\r\n",
    "    return tokenized_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-06-16 13:27:42,824] [    INFO] - Already cached /home/aistudio/.paddlenlp/models/bert-wwm-chinese/bert-wwm-chinese.pdparams\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 1, epoch: 0, batch: 0, loss: 1.730545, speed: 5.53 step/s\n",
      "global step 2, epoch: 0, batch: 1, loss: 0.624672, speed: 4.34 step/s\n",
      "global step 3, epoch: 0, batch: 2, loss: 0.512653, speed: 6.45 step/s\n",
      "global step 4, epoch: 0, batch: 3, loss: 0.272759, speed: 6.88 step/s\n",
      "global step 5, epoch: 0, batch: 4, loss: 0.740571, speed: 7.49 step/s\n",
      "global step 6, epoch: 0, batch: 5, loss: 0.377271, speed: 7.99 step/s\n",
      "global step 7, epoch: 0, batch: 6, loss: 0.300970, speed: 7.04 step/s\n",
      "global step 8, epoch: 0, batch: 7, loss: 0.273996, speed: 7.72 step/s\n",
      "global step 9, epoch: 0, batch: 8, loss: 0.136995, speed: 7.08 step/s\n",
      "global step 10, epoch: 0, batch: 9, loss: 0.241111, speed: 7.38 step/s\n",
      "global step 11, epoch: 0, batch: 10, loss: 0.049911, speed: 9.15 step/s\n",
      "global step 12, epoch: 0, batch: 11, loss: 0.252745, speed: 6.99 step/s\n",
      "global step 13, epoch: 0, batch: 12, loss: 0.455495, speed: 7.83 step/s\n",
      "global step 14, epoch: 0, batch: 13, loss: 0.265735, speed: 9.06 step/s\n",
      "global step 15, epoch: 0, batch: 14, loss: 0.121103, speed: 8.25 step/s\n",
      "global step 16, epoch: 0, batch: 15, loss: 0.084496, speed: 8.98 step/s\n",
      "global step 17, epoch: 0, batch: 16, loss: 0.099840, speed: 7.59 step/s\n",
      "global step 18, epoch: 0, batch: 17, loss: 0.094670, speed: 7.44 step/s\n",
      "global step 19, epoch: 0, batch: 18, loss: 0.166318, speed: 8.04 step/s\n",
      "global step 20, epoch: 0, batch: 19, loss: 0.055109, speed: 7.84 step/s\n",
      "global step 21, epoch: 0, batch: 20, loss: 0.140376, speed: 6.96 step/s\n",
      "global step 22, epoch: 0, batch: 21, loss: 0.129830, speed: 8.91 step/s\n",
      "global step 23, epoch: 0, batch: 22, loss: 0.147611, speed: 8.94 step/s\n",
      "global step 24, epoch: 0, batch: 23, loss: 0.100559, speed: 6.71 step/s\n",
      "global step 25, epoch: 0, batch: 24, loss: 0.184391, speed: 7.64 step/s\n",
      "global step 26, epoch: 0, batch: 25, loss: 0.113480, speed: 7.45 step/s\n",
      "global step 27, epoch: 0, batch: 26, loss: 0.115037, speed: 8.81 step/s\n",
      "global step 28, epoch: 0, batch: 27, loss: 0.087576, speed: 6.39 step/s\n",
      "global step 29, epoch: 0, batch: 28, loss: 0.179932, speed: 6.49 step/s\n",
      "global step 30, epoch: 0, batch: 29, loss: 0.091678, speed: 7.86 step/s\n",
      "global step 31, epoch: 0, batch: 30, loss: 0.091404, speed: 7.77 step/s\n",
      "global step 32, epoch: 0, batch: 31, loss: 0.174295, speed: 6.91 step/s\n",
      "global step 33, epoch: 0, batch: 32, loss: 0.045211, speed: 7.81 step/s\n",
      "global step 34, epoch: 0, batch: 33, loss: 0.092550, speed: 6.86 step/s\n",
      "global step 35, epoch: 0, batch: 34, loss: 0.095729, speed: 11.03 step/s\n",
      "global step 36, epoch: 0, batch: 35, loss: 0.051007, speed: 7.28 step/s\n",
      "global step 37, epoch: 0, batch: 36, loss: 0.063441, speed: 6.79 step/s\n",
      "global step 38, epoch: 0, batch: 37, loss: 0.060893, speed: 7.28 step/s\n",
      "global step 39, epoch: 0, batch: 38, loss: 0.212633, speed: 6.97 step/s\n",
      "global step 40, epoch: 0, batch: 39, loss: 0.022754, speed: 8.38 step/s\n",
      "global step 41, epoch: 0, batch: 40, loss: 0.057306, speed: 7.55 step/s\n",
      "global step 42, epoch: 0, batch: 41, loss: 0.040664, speed: 7.49 step/s\n",
      "global step 43, epoch: 0, batch: 42, loss: 0.039906, speed: 7.83 step/s\n",
      "global step 44, epoch: 0, batch: 43, loss: 0.002787, speed: 7.67 step/s\n",
      "global step 45, epoch: 0, batch: 44, loss: 0.030506, speed: 6.81 step/s\n",
      "global step 46, epoch: 0, batch: 45, loss: 0.025687, speed: 8.08 step/s\n",
      "global step 47, epoch: 0, batch: 46, loss: 0.071954, speed: 6.78 step/s\n",
      "global step 48, epoch: 0, batch: 47, loss: 0.095335, speed: 7.62 step/s\n",
      "global step 49, epoch: 0, batch: 48, loss: 0.096631, speed: 6.56 step/s\n",
      "global step 50, epoch: 0, batch: 49, loss: 0.042356, speed: 6.36 step/s\n",
      "global step 51, epoch: 0, batch: 50, loss: 0.056294, speed: 6.74 step/s\n",
      "global step 52, epoch: 0, batch: 51, loss: 0.018239, speed: 7.63 step/s\n",
      "global step 53, epoch: 0, batch: 52, loss: 0.025256, speed: 7.76 step/s\n",
      "global step 54, epoch: 0, batch: 53, loss: 0.045288, speed: 7.61 step/s\n",
      "global step 55, epoch: 0, batch: 54, loss: 0.071990, speed: 8.04 step/s\n",
      "global step 56, epoch: 0, batch: 55, loss: 0.135854, speed: 8.28 step/s\n",
      "global step 57, epoch: 0, batch: 56, loss: 0.029652, speed: 6.91 step/s\n",
      "global step 58, epoch: 0, batch: 57, loss: 0.046293, speed: 7.55 step/s\n",
      "global step 59, epoch: 0, batch: 58, loss: 0.037706, speed: 8.19 step/s\n",
      "global step 60, epoch: 0, batch: 59, loss: 0.033618, speed: 8.40 step/s\n",
      "global step 61, epoch: 0, batch: 60, loss: 0.061743, speed: 8.44 step/s\n",
      "global step 62, epoch: 0, batch: 61, loss: 0.036256, speed: 7.82 step/s\n",
      "global step 63, epoch: 0, batch: 62, loss: 0.062580, speed: 8.16 step/s\n",
      "global step 64, epoch: 0, batch: 63, loss: 0.099006, speed: 7.43 step/s\n",
      "global step 65, epoch: 0, batch: 64, loss: 0.034075, speed: 7.30 step/s\n",
      "global step 66, epoch: 0, batch: 65, loss: 0.054452, speed: 7.16 step/s\n",
      "global step 67, epoch: 0, batch: 66, loss: 0.027637, speed: 6.40 step/s\n",
      "global step 68, epoch: 0, batch: 67, loss: 0.042546, speed: 7.53 step/s\n",
      "global step 69, epoch: 0, batch: 68, loss: 0.012799, speed: 8.89 step/s\n",
      "global step 70, epoch: 0, batch: 69, loss: 0.028571, speed: 8.39 step/s\n",
      "global step 71, epoch: 0, batch: 70, loss: 0.010728, speed: 7.82 step/s\n",
      "global step 72, epoch: 0, batch: 71, loss: 0.017914, speed: 9.37 step/s\n",
      "global step 73, epoch: 0, batch: 72, loss: 0.022616, speed: 7.32 step/s\n",
      "global step 74, epoch: 0, batch: 73, loss: 0.015873, speed: 6.89 step/s\n",
      "global step 75, epoch: 0, batch: 74, loss: 0.017890, speed: 8.74 step/s\n",
      "global step 76, epoch: 0, batch: 75, loss: 0.240163, speed: 7.47 step/s\n",
      "global step 77, epoch: 0, batch: 76, loss: 0.008921, speed: 6.93 step/s\n",
      "global step 78, epoch: 0, batch: 77, loss: 0.017130, speed: 6.34 step/s\n",
      "global step 79, epoch: 0, batch: 78, loss: 0.034762, speed: 6.28 step/s\n",
      "global step 80, epoch: 0, batch: 79, loss: 0.081126, speed: 6.82 step/s\n",
      "global step 81, epoch: 0, batch: 80, loss: 0.015645, speed: 11.00 step/s\n",
      "global step 82, epoch: 0, batch: 81, loss: 0.011650, speed: 9.78 step/s\n",
      "global step 83, epoch: 0, batch: 82, loss: 0.049391, speed: 7.33 step/s\n",
      "global step 84, epoch: 0, batch: 83, loss: 0.028157, speed: 7.01 step/s\n",
      "global step 85, epoch: 0, batch: 84, loss: 0.018568, speed: 8.54 step/s\n",
      "global step 86, epoch: 0, batch: 85, loss: 0.038732, speed: 6.90 step/s\n",
      "global step 87, epoch: 0, batch: 86, loss: 0.015126, speed: 6.27 step/s\n",
      "global step 88, epoch: 0, batch: 87, loss: 0.054217, speed: 6.90 step/s\n",
      "global step 89, epoch: 0, batch: 88, loss: 0.065246, speed: 9.77 step/s\n",
      "global step 90, epoch: 0, batch: 89, loss: 0.017518, speed: 7.71 step/s\n",
      "global step 91, epoch: 0, batch: 90, loss: 0.052295, speed: 7.73 step/s\n",
      "global step 92, epoch: 0, batch: 91, loss: 0.057033, speed: 7.53 step/s\n",
      "global step 93, epoch: 0, batch: 92, loss: 0.075016, speed: 6.31 step/s\n",
      "global step 94, epoch: 0, batch: 93, loss: 0.048878, speed: 7.39 step/s\n",
      "global step 95, epoch: 0, batch: 94, loss: 0.009867, speed: 9.33 step/s\n",
      "global step 96, epoch: 0, batch: 95, loss: 0.039740, speed: 8.84 step/s\n",
      "global step 97, epoch: 0, batch: 96, loss: 0.063239, speed: 10.03 step/s\n",
      "global step 98, epoch: 0, batch: 97, loss: 0.116075, speed: 8.52 step/s\n",
      "global step 99, epoch: 0, batch: 98, loss: 0.028308, speed: 8.65 step/s\n",
      "global step 100, epoch: 0, batch: 99, loss: 0.035400, speed: 9.52 step/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/tensor/creation.py:125: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  if data.dtype == np.object:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval loss: 0.000115, precision: 0.938020, recall: 0.948728, f1: 0.943344\r"
     ]
    }
   ],
   "source": [
    "label_list = train_ds.label_list\r\n",
    "label_num = len(label_list)\r\n",
    "no_entity_id = label_num - 1\r\n",
    "\r\n",
    "trans_func = partial(\r\n",
    "        tokenize_and_align_labels,\r\n",
    "        tokenizer=tokenizer,\r\n",
    "        no_entity_id=no_entity_id,\r\n",
    "        max_seq_len=128)\r\n",
    "\r\n",
    "train_ds = train_ds.map(trans_func)\r\n",
    "\r\n",
    "ignore_label = -100\r\n",
    "\r\n",
    "batchify_fn = lambda samples, fn=Dict({\r\n",
    "        'input_ids': Pad(axis=0, pad_val=tokenizer.pad_token_id, dtype='int32'),  # input\r\n",
    "        'token_type_ids': Pad(axis=0, pad_val=tokenizer.pad_token_type_id, dtype='int32'),  # segment\r\n",
    "        'seq_len': Stack(dtype='int64'),  # seq_len\r\n",
    "        'labels': Pad(axis=0, pad_val=ignore_label, dtype='int64')  # label\r\n",
    "    }): fn(samples)\r\n",
    "\r\n",
    "train_batch_sampler = paddle.io.DistributedBatchSampler(\r\n",
    "        train_ds, batch_size=8, shuffle=True, drop_last=True)\r\n",
    "\r\n",
    "train_data_loader = DataLoader(\r\n",
    "        dataset=train_ds,\r\n",
    "        collate_fn=batchify_fn,\r\n",
    "        num_workers=0,\r\n",
    "        batch_sampler=train_batch_sampler,\r\n",
    "        return_list=True)\r\n",
    "\r\n",
    "test_ds = test_ds.map(trans_func)\r\n",
    "\r\n",
    "test_data_loader = DataLoader(\r\n",
    "        dataset=test_ds,\r\n",
    "        collate_fn=batchify_fn,\r\n",
    "        num_workers=0,\r\n",
    "        batch_size=8,\r\n",
    "        return_list=True)\r\n",
    "\r\n",
    "    # Define the model netword and its loss\r\n",
    "model = BertForTokenClassification.from_pretrained(\r\n",
    "        'bert-wwm-chinese', num_classes=label_num)\r\n",
    "if paddle.distributed.get_world_size() > 1:\r\n",
    "        model = paddle.DataParallel(model)\r\n",
    "\r\n",
    "num_training_steps = -1 if -1 > 0 else len(\r\n",
    "        train_data_loader) * 3\r\n",
    "\r\n",
    "lr_scheduler = LinearDecayWithWarmup(5e-5, num_training_steps,0)\r\n",
    "\r\n",
    "    # Generate parameter names needed to perform weight decay.\r\n",
    "    # All bias and LayerNorm parameters are excluded.\r\n",
    "decay_params = [\r\n",
    "        p.name for n, p in model.named_parameters()\r\n",
    "        if not any(nd in n for nd in [\"bias\", \"norm\"])\r\n",
    "    ]\r\n",
    "optimizer = paddle.optimizer.AdamW(\r\n",
    "        learning_rate=lr_scheduler,\r\n",
    "        epsilon=1e-8,\r\n",
    "        parameters=model.parameters(),\r\n",
    "        weight_decay=0.0,\r\n",
    "        apply_decay_param_fun=lambda x: x in decay_params)\r\n",
    "\r\n",
    "loss_fct = paddle.nn.loss.CrossEntropyLoss(ignore_index=ignore_label)\r\n",
    "\r\n",
    "metric = ChunkEvaluator(label_list=label_list)\r\n",
    "\r\n",
    "global_step = 0\r\n",
    "last_step = 3 * len(train_data_loader)\r\n",
    "tic_train = time.time()\r\n",
    "for epoch in range(3):\r\n",
    "    for step, batch in enumerate(train_data_loader):\r\n",
    "        global_step += 1\r\n",
    "        input_ids, token_type_ids, _, labels = batch\r\n",
    "        logits = model(input_ids, token_type_ids)\r\n",
    "        loss = loss_fct(logits, labels)\r\n",
    "        avg_loss = paddle.mean(loss)\r\n",
    "        if global_step % 1 == 0:\r\n",
    "            print(\r\n",
    "                    \"global step %d, epoch: %d, batch: %d, loss: %f, speed: %.2f step/s\"\r\n",
    "                    % (global_step, epoch, step, avg_loss,\r\n",
    "                       1 / (time.time() - tic_train)))\r\n",
    "            tic_train = time.time()\r\n",
    "        avg_loss.backward()\r\n",
    "        optimizer.step()\r\n",
    "        lr_scheduler.step()\r\n",
    "        optimizer.clear_grad()\r\n",
    "        if global_step % 100 == 0 or global_step == last_step:\r\n",
    "            if paddle.distributed.get_rank() == 0:\r\n",
    "                evaluate(model, loss_fct, metric, test_data_loader,\r\n",
    "                             label_num)\r\n",
    "                # paddle.save(model.state_dict(),\r\n",
    "                #                 os.path.join(args.output_dir,\r\n",
    "                #                              \"model_%d.pdparams\" % global_step))\r\n",
    "                paddle.save(model.state_dict(),\r\n",
    "                './ernie_result/model_%d.pdparams' % step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results have been saved in the file: ernie_results.txt, some examples are shown below: \n",
      "('中共中央', 'ORG')('致', 'O')('中国致公党十一大', 'ORG')('的', 'O')('贺', 'O')('词', 'O')('各', 'O')('位', 'O')('代', 'O')('表', 'O')('、', 'O')('各', 'O')('位', 'O')('同', 'O')('志', 'O')('：', 'O')('在', 'O')('中国致公党第十一次全国代表大会', 'ORG')('隆', 'O')('重', 'O')('召', 'O')('开', 'O')('之', 'O')('际', 'O')('，', 'O')('中国共产党中央委员会', 'ORG')('谨', 'O')('向', 'O')('大', 'O')('会', 'O')('表', 'O')('示', 'O')('热', 'O')('烈', 'O')('的', 'O')('祝', 'O')('贺', 'O')('，', 'O')('向', 'O')('致公党', 'ORG')('的', 'O')('同', 'O')('志', 'O')('们', 'O')('致', 'O')('以', 'O')('亲', 'O')('切', 'O')('的', 'O')('问', 'O')('候', 'O')('！', 'O')\n",
      "('在', 'O')('过', 'O')('去', 'O')('的', 'O')('五', 'O')('年', 'O')('中', 'O')('，', 'O')('致公党', 'ORG')('在', 'O')('邓小平', 'PER')('理', 'O')('论', 'O')('指', 'O')('引', 'O')('下', 'O')('，', 'O')('遵', 'O')('循', 'O')('社', 'O')('会', 'O')('主', 'O')('义', 'O')('初', 'O')('级', 'O')('阶', 'O')('段', 'O')('的', 'O')('基', 'O')('本', 'O')('路', 'O')('线', 'O')('，', 'O')('努', 'O')('力', 'O')('实', 'O')('践', 'O')('致公党十大', 'ORG')('提', 'O')('出', 'O')('的', 'O')('发', 'O')('挥', 'O')('参', 'O')('政', 'O')('党', 'O')('职', 'O')('能', 'O')('、', 'O')('加', 'O')('强', 'O')('自', 'O')('身', 'O')('建', 'O')('设', 'O')('的', 'O')('基', 'O')('本', 'O')('任', 'O')('务', 'O')('。', 'O')\n",
      "('高', 'O')('举', 'O')('爱', 'O')('国', 'O')('主', 'O')('义', 'O')('和', 'O')('社', 'O')('会', 'O')('主', 'O')('义', 'O')('两', 'O')('面', 'O')('旗', 'O')('帜', 'O')('，', 'O')('团', 'O')('结', 'O')('全', 'O')('体', 'O')('成', 'O')('员', 'O')('以', 'O')('及', 'O')('所', 'O')('联', 'O')('系', 'O')('的', 'O')('归', 'O')('侨', 'O')('、', 'O')('侨', 'O')('眷', 'O')('，', 'O')('发', 'O')('扬', 'O')('爱', 'O')('国', 'O')('革', 'O')('命', 'O')('的', 'O')('光', 'O')('荣', 'O')('传', 'O')('统', 'O')('，', 'O')('为', 'O')('统', 'O')('一', 'O')('祖', 'O')('国', 'O')('、', 'O')('振', 'O')('兴', 'O')('中华', 'LOC')('而', 'O')('努', 'O')('力', 'O')('奋', 'O')('斗', 'O')('；', 'O')('紧', 'O')('紧', 'O')('围', 'O')('绕', 'O')('国', 'O')('家', 'O')('的', 'O')('中', 'O')('心', 'O')('工', 'O')('作', 'O')('，', 'O')('联', 'O')('系', 'O')('改', 'O')('革', 'O')('和', 'O')('建', 'O')('设', 'O')('中', 'O')('的', 'O')('重', 'O')('大', 'O')('问', 'O')('题', 'O')('以', 'O')('及', 'O')('人', 'O')('民', 'O')('群', 'O')('众', 'O')('普', 'O')('遍', 'O')('关', 'O')('心', 'O')('的', 'O')('社', 'O')('会', 'O')('问', 'O')('题', 'O')('，', 'O')('深', 'O')('入', 'O')('开', 'O')('展', 'O')('调', 'O')('查', 'O')('研', 'O')('究', 'O')('，', 'O')('就', 'O')('经', 'O')('济', 'O')('建', 'O')('设', 'O')('、', 'O')('侨', 'O')('务', 'O')('政', 'O')('策', 'O')('、', 'O')('文', 'O')('教', 'O')('卫', 'O')('生', 'O')\n",
      "('在', 'O')('此', 'O')('，', 'O')('中共中央', 'ORG')('谨', 'O')('向', 'O')('致公党中央', 'ORG')('以', 'O')('及', 'O')('全', 'O')('体', 'O')('成', 'O')('员', 'O')('致', 'O')('以', 'O')('崇', 'O')('高', 'O')('的', 'O')('敬', 'O')('意', 'O')('！', 'O')\n",
      "('不', 'O')('久', 'O')('前', 'O')('，', 'O')('中国共产党', 'ORG')('召', 'O')('开', 'O')('了', 'O')('举', 'O')('世', 'O')('瞩', 'O')('目', 'O')('的', 'O')('第十五次全国代表大会', 'O')('。', 'O')\n",
      "('这', 'O')('次', 'O')('代', 'O')('表', 'O')('大', 'O')('会', 'O')('是', 'O')('在', 'O')('中国', 'LOC')('改', 'O')('革', 'O')('开', 'O')('放', 'O')('和', 'O')('社', 'O')('会', 'O')('主', 'O')('义', 'O')('现', 'O')('代', 'O')('化', 'O')('建', 'O')('设', 'O')('发', 'O')('展', 'O')('的', 'O')('关', 'O')('键', 'O')('时', 'O')('刻', 'O')('召', 'O')('开', 'O')('的', 'O')('历', 'O')('史', 'O')('性', 'O')('会', 'O')('议', 'O')('。', 'O')\n",
      "('大', 'O')('会', 'O')('高', 'O')('举', 'O')('邓小平', 'PER')('理', 'O')('论', 'O')('伟', 'O')('大', 'O')('旗', 'O')('帜', 'O')('，', 'O')('回', 'O')('顾', 'O')('一', 'O')('个', 'O')('世', 'O')('纪', 'O')('以', 'O')('来', 'O')('中国', 'LOC')('人', 'O')('民', 'O')('的', 'O')('奋', 'O')('斗', 'O')('历', 'O')('史', 'O')('，', 'O')('展', 'O')('望', 'O')('下', 'O')('个', 'O')('世', 'O')('纪', 'O')('５', 'O')('０', 'O')('年', 'O')('的', 'O')('发', 'O')('展', 'O')('前', 'O')('景', 'O')('，', 'O')('认', 'O')('真', 'O')('总', 'O')('结', 'O')('了', 'O')('中共', 'ORG')('十', 'O')('一', 'O')('届', 'O')('三', 'O')('中', 'O')('全', 'O')('会', 'O')('以', 'O')('来', 'O')('特', 'O')('别', 'O')('是', 'O')('十四大', 'ORG')('以', 'O')('来', 'O')('的', 'O')('实', 'O')('践', 'O')('经', 'O')('验', 'O')('，', 'O')('对', 'O')('中国', 'LOC')('改', 'O')('革', 'O')('开', 'O')('放', 'O')('和', 'O')('社', 'O')('会', 'O')('主', 'O')('义', 'O')('现', 'O')('代', 'O')('化', 'O')('建', 'O')('设', 'O')('跨', 'O')('世', 'O')('纪', 'O')('的', 'O')('发', 'O')('展', 'O')('作', 'O')('出', 'O')('了', 'O')('全', 'O')('面', 'O')('部', 'O')('署', 'O')('。', 'O')\n",
      "('这', 'O')('次', 'O')('大', 'O')('会', 'O')('对', 'O')('于', 'O')('动', 'O')('员', 'O')('全', 'O')('党', 'O')('和', 'O')('全', 'O')('国', 'O')('各', 'O')('族', 'O')('人', 'O')('民', 'O')('，', 'O')('解', 'O')('放', 'O')('思', 'O')('想', 'O')('，', 'O')('实', 'O')('事', 'O')('求', 'O')('是', 'O')('，', 'O')('抓', 'O')('住', 'O')('有', 'O')('利', 'O')('时', 'O')('机', 'O')('，', 'O')('继', 'O')('续', 'O')('开', 'O')('拓', 'O')('前', 'O')('进', 'O')('，', 'O')('把', 'O')('建', 'O')('设', 'O')('有', 'O')('中国', 'LOC')('特', 'O')('色', 'O')('社', 'O')('会', 'O')('主', 'O')('义', 'O')('伟', 'O')('大', 'O')('事', 'O')('业', 'O')('全', 'O')('面', 'O')('推', 'O')('向', 'O')('２', 'O')('１', 'O')('世', 'O')('纪', 'O')('，', 'O')('具', 'O')('有', 'O')('极', 'O')('其', 'O')('重', 'O')('大', 'O')('和', 'O')('深', 'O')('远', 'O')('的', 'O')('意', 'O')('义', 'O')('。', 'O')\n",
      "('当', 'O')('前', 'O')('，', 'O')('在', 'O')('中共十五大', 'ORG')('精', 'O')('神', 'O')('的', 'O')('指', 'O')('引', 'O')('下', 'O')('，', 'O')('在', 'O')('以', 'O')('江泽民', 'PER')('同', 'O')('志', 'O')('为', 'O')('核', 'O')('心', 'O')('的', 'O')('中共中央', 'ORG')('领', 'O')('导', 'O')('下', 'O')('，', 'O')('全', 'O')('党', 'O')('和', 'O')('全', 'O')('国', 'O')('各', 'O')('族', 'O')('人', 'O')('民', 'O')('正', 'O')('高', 'O')('举', 'O')('邓小平', 'PER')('理', 'O')('论', 'O')('伟', 'O')('大', 'O')('旗', 'O')('帜', 'O')('，', 'O')('同', 'O')('心', 'O')('同', 'O')('德', 'O')('，', 'O')('团', 'O')('结', 'O')('奋', 'O')('斗', 'O')('，', 'O')('沿', 'O')('着', 'O')('建', 'O')('设', 'O')('有', 'O')('中国', 'LOC')('特', 'O')('色', 'O')('的', 'O')('社', 'O')('会', 'O')('主', 'O')('义', 'O')('道', 'O')('路', 'O')('阔', 'O')('步', 'O')('前', 'O')('进', 'O')('。', 'O')\n",
      "('实', 'O')('现', 'O')('建', 'O')('设', 'O')('有', 'O')('中国', 'LOC')('特', 'O')('色', 'O')('社', 'O')('会', 'O')('主', 'O')('义', 'O')('的', 'O')('宏', 'O')('伟', 'O')('目', 'O')('标', 'O')('，', 'O')('是', 'O')('中国共产党', 'ORG')('和', 'O')('作', 'O')('为', 'O')('参', 'O')('政', 'O')('党', 'O')('的', 'O')('各', 'O')('民', 'O')('主', 'O')('党', 'O')('派', 'O')('共', 'O')('同', 'O')('肩', 'O')('负', 'O')('的', 'O')('历', 'O')('史', 'O')('使', 'O')('命', 'O')('。', 'O')\n"
     ]
    }
   ],
   "source": [
    "# preds = predict(model, test_data_loader, test_ds, label_num)\r\n",
    "# file_path = \"ernie_results.txt\"\r\n",
    "# with open(file_path, \"w\", encoding=\"utf8\") as fout:\r\n",
    "#     fout.write(\"\\n\".join(preds))\r\n",
    "# # Print some examples\r\n",
    "# print(\r\n",
    "#     \"The results have been saved in the file: %s, some examples are shown below: \"\r\n",
    "#     % file_path)\r\n",
    "# print(\"\\n\".join(preds[:10]))\r\n",
    "def parse_decodes(input_words, id2label, decodes, lens):\r\n",
    "    decodes = [x for batch in decodes for x in batch]\r\n",
    "    lens = [x for batch in lens for x in batch]\r\n",
    "\r\n",
    "    outputs = []\r\n",
    "    for idx, end in enumerate(lens):\r\n",
    "        sent = \"\".join(input_words[idx]['tokens'])\r\n",
    "        tags = [id2label[x] for x in decodes[idx][1:end]]\r\n",
    "        sent_out = []\r\n",
    "        tags_out = []\r\n",
    "        words = \"\"\r\n",
    "        for s, t in zip(sent, tags):\r\n",
    "            if t.startswith('B-') or t == 'O':\r\n",
    "                if len(words):\r\n",
    "                    sent_out.append(words)\r\n",
    "                if t.startswith('B-'):\r\n",
    "                    tags_out.append(t.split('-')[1])\r\n",
    "                else:\r\n",
    "                    tags_out.append(t)\r\n",
    "                words = s\r\n",
    "            else:\r\n",
    "                words += s\r\n",
    "        if len(sent_out) < len(tags_out):\r\n",
    "            sent_out.append(words)\r\n",
    "        outputs.append(''.join(\r\n",
    "            [str((s, t)) for s, t in zip(sent_out, tags_out)]))\r\n",
    "    return outputs\r\n",
    "train_ds, predict_ds = load_dataset(\r\n",
    "        'msra_ner', splits=('train', 'test'), lazy=False)\r\n",
    "raw_data = predict_ds.data\r\n",
    "id2label = dict(enumerate(predict_ds.label_list))\r\n",
    "\r\n",
    "predict_ds = predict_ds.map(trans_func)\r\n",
    "predict_data_loader = DataLoader(\r\n",
    "    dataset=predict_ds,\r\n",
    "    collate_fn=batchify_fn,\r\n",
    "    num_workers=0,\r\n",
    "    batch_size=8,\r\n",
    "    return_list=True)\r\n",
    "\r\n",
    "pred_list = []\r\n",
    "len_list = []\r\n",
    "for step, batch in enumerate(predict_data_loader):\r\n",
    "    input_ids, token_type_ids, length, labels = batch\r\n",
    "    logits = model(input_ids, token_type_ids)\r\n",
    "    pred = paddle.argmax(logits, axis=-1)\r\n",
    "    pred_list.append(pred.numpy())\r\n",
    "    len_list.append(length.numpy())\r\n",
    "preds = parse_decodes(raw_data, id2label, pred_list, len_list)\r\n",
    "\r\n",
    "file_path = \"ernie_results.txt\"\r\n",
    "with open(file_path, \"w\", encoding=\"utf8\") as fout:\r\n",
    "        fout.write(\"\\n\".join(preds))\r\n",
    "    # Print some examples\r\n",
    "print(\r\n",
    "        \"The results have been saved in the file: %s, some examples are shown below: \"\r\n",
    "        % file_path)\r\n",
    "print(\"\\n\".join(preds[:10]))\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 加载自定义数据集\n",
    "\n",
    "推荐使用MapDataset()自定义数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_a\tlabel\r\n",
      "黑\u0002龙\u0002江\u0002省\u0002双\u0002鸭\u0002山\u0002市\u0002尖\u0002山\u0002区\u0002八\u0002马\u0002路\u0002与\u0002东\u0002平\u0002行\u0002路\u0002交\u0002叉\u0002口\u0002北\u00024\u00020\u0002米\u0002韦\u0002业\u0002涛\u00021\u00028\u00026\u00020\u00020\u00020\u00020\u00029\u00021\u00027\u00022\tA1-B\u0002A1-I\u0002A1-I\u0002A1-I\u0002A2-B\u0002A2-I\u0002A2-I\u0002A2-I\u0002A3-B\u0002A3-I\u0002A3-I\u0002A4-B\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002P-B\u0002P-I\u0002P-I\u0002T-B\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\r\n",
      "广\u0002西\u0002壮\u0002族\u0002自\u0002治\u0002区\u0002桂\u0002林\u0002市\u0002雁\u0002山\u0002区\u0002雁\u0002山\u0002镇\u0002西\u0002龙\u0002村\u0002老\u0002年\u0002活\u0002动\u0002中\u0002心\u00021\u00027\u00026\u00021\u00020\u00023\u00024\u00028\u00028\u00028\u00028\u0002羊\u0002卓\u0002卫\tA1-B\u0002A1-I\u0002A1-I\u0002A1-I\u0002A1-I\u0002A1-I\u0002A1-I\u0002A2-B\u0002A2-I\u0002A2-I\u0002A3-B\u0002A3-I\u0002A3-I\u0002A4-B\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002T-B\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002P-B\u0002P-I\u0002P-I\r\n",
      "1\u00025\u00026\u00025\u00022\u00028\u00026\u00024\u00025\u00026\u00021\u0002河\u0002南\u0002省\u0002开\u0002封\u0002市\u0002顺\u0002河\u0002回\u0002族\u0002区\u0002顺\u0002河\u0002区\u0002公\u0002园\u0002路\u00023\u00022\u0002号\u0002赵\u0002本\u0002山\tT-B\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002A1-B\u0002A1-I\u0002A1-I\u0002A2-B\u0002A2-I\u0002A2-I\u0002A3-B\u0002A3-I\u0002A3-I\u0002A3-I\u0002A3-I\u0002A4-B\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002P-B\u0002P-I\u0002P-I\r\n",
      "河\u0002北\u0002省\u0002唐\u0002山\u0002市\u0002玉\u0002田\u0002县\u0002无\u0002终\u0002大\u0002街\u00021\u00025\u00029\u0002号\u00021\u00028\u00026\u00021\u00024\u00022\u00025\u00023\u00020\u00025\u00028\u0002尚\u0002汉\u0002生\tA1-B\u0002A1-I\u0002A1-I\u0002A2-B\u0002A2-I\u0002A2-I\u0002A3-B\u0002A3-I\u0002A3-I\u0002A4-B\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002T-B\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002P-B\u0002P-I\u0002P-I\r\n"
     ]
    }
   ],
   "source": [
    "# 下载并解压数据集\r\n",
    "from paddle.utils.download import get_path_from_url\r\n",
    "URL = \"https://paddlenlp.bj.bcebos.com/paddlenlp/datasets/waybill.tar.gz\"\r\n",
    "get_path_from_url(URL, \"./\")\r\n",
    "\r\n",
    "# 查看预测的数据\r\n",
    "!head -n 5 data/test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\r\n",
    "from functools import partial\r\n",
    "\r\n",
    "import paddle\r\n",
    "from paddlenlp.datasets import MapDataset\r\n",
    "from paddlenlp.data import Stack, Tuple, Pad\r\n",
    "from paddlenlp.transformers import ErnieTokenizer, ErnieForTokenClassification\r\n",
    "from paddlenlp.metrics import ChunkEvaluator\r\n",
    "from utils import convert_example, evaluate, predict, load_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_dataset(datafiles):\r\n",
    "    def read(data_path):\r\n",
    "        with open(data_path, 'r', encoding='utf-8') as fp:\r\n",
    "            next(fp)  # Skip header\r\n",
    "            for line in fp.readlines():\r\n",
    "                words, labels = line.strip('\\n').split('\\t')\r\n",
    "                words = words.split('\\002')\r\n",
    "                labels = labels.split('\\002')\r\n",
    "                yield words, labels\r\n",
    "\r\n",
    "    if isinstance(datafiles, str):\r\n",
    "        return MapDataset(list(read(datafiles)))\r\n",
    "    elif isinstance(datafiles, list) or isinstance(datafiles, tuple):\r\n",
    "        return [MapDataset(list(read(datafile))) for datafile in datafiles]\r\n",
    "\r\n",
    "# Create dataset, tokenizer and dataloader.\r\n",
    "train_ds, dev_ds, test_ds = load_dataset(datafiles=(\r\n",
    "        './data/train.txt', './data/dev.txt', './data/test.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def load_dataset(datafiles):\n",
    "#     def read(data_path):\n",
    "#         with open(data_path, 'r', encoding='utf-8') as fp:\n",
    "#             next(fp)  # Skip header\n",
    "#             for line in fp.readlines():\n",
    "#                 words, labels = line.strip('\\n').split('\\t')\n",
    "#                 words = words.split('\\002')\n",
    "#                 labels = labels.split('\\002')\n",
    "#                 yield words, labels\n",
    "\n",
    "#     if isinstance(datafiles, str):\n",
    "#         return MapDataset(list(read(datafiles)))\n",
    "#     elif isinstance(datafiles, list) or isinstance(datafiles, tuple):\n",
    "#         return [MapDataset(list(read(datafile))) for datafile in datafiles]\n",
    "\n",
    "# # Create dataset, tokenizer and dataloader.\n",
    "# train_ds, test_ds = load_dataset(datafiles=(\n",
    "#         './msra_ner/train.tsv', './msra_ner/test.tsv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['1', '6', '6', '2', '0', '2', '0', '0', '0', '7', '7', '宣', '荣', '嗣', '甘', '肃', '省', '白', '银', '市', '会', '宁', '县', '河', '畔', '镇', '十', '字', '街', '金', '海', '超', '市', '西', '行', '5', '0', '米'], ['T-B', 'T-I', 'T-I', 'T-I', 'T-I', 'T-I', 'T-I', 'T-I', 'T-I', 'T-I', 'T-I', 'P-B', 'P-I', 'P-I', 'A1-B', 'A1-I', 'A1-I', 'A2-B', 'A2-I', 'A2-I', 'A3-B', 'A3-I', 'A3-I', 'A4-B', 'A4-I', 'A4-I', 'A4-I', 'A4-I', 'A4-I', 'A4-I', 'A4-I', 'A4-I', 'A4-I', 'A4-I', 'A4-I', 'A4-I', 'A4-I', 'A4-I'])\n",
      "(['1', '3', '5', '5', '2', '6', '6', '4', '3', '0', '7', '姜', '骏', '炜', '云', '南', '省', '德', '宏', '傣', '族', '景', '颇', '族', '自', '治', '州', '盈', '江', '县', '平', '原', '镇', '蜜', '回', '路', '下', '段'], ['T-B', 'T-I', 'T-I', 'T-I', 'T-I', 'T-I', 'T-I', 'T-I', 'T-I', 'T-I', 'T-I', 'P-B', 'P-I', 'P-I', 'A1-B', 'A1-I', 'A1-I', 'A2-B', 'A2-I', 'A2-I', 'A2-I', 'A2-I', 'A2-I', 'A2-I', 'A2-I', 'A2-I', 'A2-I', 'A3-B', 'A3-I', 'A3-I', 'A4-B', 'A4-I', 'A4-I', 'A4-I', 'A4-I', 'A4-I', 'A4-I', 'A4-I'])\n",
      "(['内', '蒙', '古', '自', '治', '区', '赤', '峰', '市', '阿', '鲁', '科', '尔', '沁', '旗', '汉', '林', '西', '街', '路', '南', '1', '3', '7', '0', '1', '0', '8', '5', '3', '9', '0', '那', '峥'], ['A1-B', 'A1-I', 'A1-I', 'A1-I', 'A1-I', 'A1-I', 'A2-B', 'A2-I', 'A2-I', 'A3-B', 'A3-I', 'A3-I', 'A3-I', 'A3-I', 'A3-I', 'A4-B', 'A4-I', 'A4-I', 'A4-I', 'A4-I', 'A4-I', 'T-B', 'T-I', 'T-I', 'T-I', 'T-I', 'T-I', 'T-I', 'T-I', 'T-I', 'T-I', 'T-I', 'P-B', 'P-I'])\n",
      "(['广', '东', '省', '梅', '州', '市', '大', '埔', '县', '茶', '阳', '镇', '胜', '利', '路', '1', '3', '6', '0', '1', '3', '2', '8', '1', '7', '3', '张', '铱'], ['A1-B', 'A1-I', 'A1-I', 'A2-B', 'A2-I', 'A2-I', 'A3-B', 'A3-I', 'A3-I', 'A4-B', 'A4-I', 'A4-I', 'A4-I', 'A4-I', 'A4-I', 'T-B', 'T-I', 'T-I', 'T-I', 'T-I', 'T-I', 'T-I', 'T-I', 'T-I', 'T-I', 'T-I', 'P-B', 'P-I'])\n",
      "(['新', '疆', '维', '吾', '尔', '自', '治', '区', '阿', '克', '苏', '地', '区', '阿', '克', '苏', '市', '步', '行', '街', '1', '0', '号', '1', '5', '8', '1', '0', '7', '8', '9', '3', '7', '8', '慕', '东', '霖'], ['A1-B', 'A1-I', 'A1-I', 'A1-I', 'A1-I', 'A1-I', 'A1-I', 'A1-I', 'A2-B', 'A2-I', 'A2-I', 'A2-I', 'A2-I', 'A3-B', 'A3-I', 'A3-I', 'A3-I', 'A4-B', 'A4-I', 'A4-I', 'A4-I', 'A4-I', 'A4-I', 'T-B', 'T-I', 'T-I', 'T-I', 'T-I', 'T-I', 'T-I', 'T-I', 'T-I', 'T-I', 'T-I', 'P-B', 'P-I', 'P-I'])\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(train_ds[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "每条数据包含一句文本和这个文本中每个汉字以及数字对应的label标签。\n",
    "\n",
    "之后，还需要对输入句子进行数据处理，如切词，映射词表id等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 数据处理\n",
    "\n",
    "预训练模型ERNIE对中文数据的处理是以字为单位。PaddleNLP对于各种预训练模型已经内置了相应的tokenizer。指定想要使用的模型名字即可加载对应的tokenizer。\n",
    "\n",
    "tokenizer作用为将原始输入文本转化成模型model可以接受的输入数据形式。\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://bj.bcebos.com/paddlehub/paddlehub-img/ernie_network_1.png\" hspace='10'/> <br />\n",
    "</p>\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://bj.bcebos.com/paddlehub/paddlehub-img/ernie_network_2.png\" hspace='10'/> <br />\n",
    "</p>\n",
    "<br><center>图3：ERNIE模型示意图</center></br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# label_vocab = load_dict('./msra_ner/label_map.json')\n",
    "# # tokenizer = ErnieTokenizer.from_pretrained('ernie-1.0')\n",
    "# tokenizer = BertTokenizer.from_pretrained(\"bert-wwm-chinese\")\n",
    "# trans_func = partial(convert_example, tokenizer=tokenizer, label_vocab=label_vocab)\n",
    "\n",
    "# train_ds.map(trans_func)\n",
    "# # dev_ds.map(trans_func)\n",
    "# test_ds.map(trans_func)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-06-16 16:37:33,955] [    INFO] - Downloading vocab.txt from https://paddlenlp.bj.bcebos.com/models/transformers/ernie/vocab.txt\n",
      "100%|██████████| 90/90 [00:00<00:00, 3559.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([1, 208, 515, 515, 249, 540, 249, 540, 540, 540, 589, 589, 803, 838, 2914, 1222, 1734, 244, 368, 797, 99, 32, 863, 308, 457, 2778, 484, 167, 436, 930, 192, 233, 634, 99, 213, 40, 317, 540, 256, 2], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 40, [12, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 1, 1, 4, 5, 5, 6, 7, 7, 8, 9, 9, 10, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12])\n"
     ]
    }
   ],
   "source": [
    "label_vocab = load_dict('./data/tag.dic')\r\n",
    "tokenizer = ErnieTokenizer.from_pretrained('ernie-1.0')\r\n",
    "\r\n",
    "trans_func = partial(convert_example, tokenizer=tokenizer, label_vocab=label_vocab)\r\n",
    "\r\n",
    "train_ds.map(trans_func)\r\n",
    "dev_ds.map(trans_func)\r\n",
    "test_ds.map(trans_func)\r\n",
    "print (train_ds[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 数据读入\n",
    "\n",
    "使用`paddle.io.DataLoader`接口多线程异步加载数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ignore_label = -1\n",
    "batchify_fn = lambda samples, fn=Tuple(\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_id),  # input_ids\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_type_id),  # token_type_ids\n",
    "    Stack(),  # seq_len\n",
    "    Pad(axis=0, pad_val=ignore_label)  # labels\n",
    "): fn(samples)\n",
    "\n",
    "train_loader = paddle.io.DataLoader(\n",
    "    dataset=train_ds,\n",
    "    batch_size=36,\n",
    "    return_list=True,\n",
    "    collate_fn=batchify_fn)\n",
    "dev_loader = paddle.io.DataLoader(\n",
    "    dataset=dev_ds,\n",
    "    batch_size=36,\n",
    "    return_list=True,\n",
    "    collate_fn=batchify_fn)\n",
    "test_loader = paddle.io.DataLoader(\n",
    "    dataset=test_ds,\n",
    "    batch_size=36,\n",
    "    return_list=True,\n",
    "    collate_fn=batchify_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## PaddleNLP一键加载预训练模型\n",
    "\n",
    "\n",
    "快递单信息抽取本质是一个序列标注任务，PaddleNLP对于各种预训练模型已经内置了对于下游任务文本分类Fine-tune网络。以下教程以ERNIE为预训练模型完成序列标注任务。\n",
    "\n",
    "`paddlenlp.transformers.ErnieForTokenClassification()`一行代码即可加载预训练模型ERNIE用于序列标注任务的fine-tune网络。其在ERNIE模型后拼接上一个全连接网络进行分类。\n",
    "\n",
    "`paddlenlp.transformers.ErnieForTokenClassification.from_pretrained()`方法只需指定想要使用的模型名称和文本分类的类别数即可完成定义模型网络。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-06-16 16:37:36,395] [    INFO] - Downloading https://paddlenlp.bj.bcebos.com/models/transformers/ernie/ernie_v1_chn_base.pdparams and saved to /home/aistudio/.paddlenlp/models/ernie-1.0\n",
      "[2021-06-16 16:37:36,397] [    INFO] - Downloading ernie_v1_chn_base.pdparams from https://paddlenlp.bj.bcebos.com/models/transformers/ernie/ernie_v1_chn_base.pdparams\n",
      "100%|██████████| 392507/392507 [00:07<00:00, 53042.13it/s]\n"
     ]
    }
   ],
   "source": [
    "# Define the model netword and its loss\n",
    "model = ErnieForTokenClassification.from_pretrained(\"ernie-1.0\", num_classes=len(label_vocab))\n",
    "# model = BertForSequenceClassification.from_pretrained(\"bert-wwm-chinese\", num_classes=label_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "PaddleNLP不仅支持ERNIE预训练模型，还支持BERT、RoBERTa、Electra等预训练模型。\n",
    "下表汇总了目前PaddleNLP支持的各类预训练模型。您可以使用PaddleNLP提供的模型，完成文本分类、序列标注、问答等任务。同时我们提供了众多预训练模型的参数权重供用户使用，其中包含了二十多种中文语言模型的预训练权重。中文的预训练模型有`bert-base-chinese, bert-wwm-chinese, bert-wwm-ext-chinese, ernie-1.0, ernie-tiny, gpt2-base-cn, roberta-wwm-ext, roberta-wwm-ext-large, rbt3, rbtl3, chinese-electra-base, chinese-electra-small, chinese-xlnet-base, chinese-xlnet-mid, chinese-xlnet-large, unified_transformer-12L-cn, unified_transformer-12L-cn-luge`等。\n",
    "\n",
    "更多预训练模型参考：[PaddleNLP Transformer API](https://github.com/PaddlePaddle/PaddleNLP/blob/develop/docs/transformers.md)。\n",
    "\n",
    "更多预训练模型fine-tune下游任务使用方法，请参考：[examples](https://github.com/PaddlePaddle/PaddleNLP/tree/develop/examples)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 设置Fine-Tune优化策略，模型配置\n",
    "适用于ERNIE/BERT这类Transformer模型的迁移优化学习率策略为warmup的动态学习率。\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://ai-studio-static-online.cdn.bcebos.com/2bc624280a614a80b5449773192be460f195b13af89e4e5cbaf62bf6ac16de2c\" width=\"40%\" height=\"30%\"/> <br />\n",
    "</p><br><center>图4：动态学习率示意图</center></br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "metric = ChunkEvaluator(label_list=label_vocab.keys(), suffix=True)\n",
    "loss_fn = paddle.nn.loss.CrossEntropyLoss(ignore_index=ignore_label)\n",
    "optimizer = paddle.optimizer.AdamW(learning_rate=2e-5, parameters=model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 模型训练与评估\n",
    "\n",
    "\n",
    "模型训练的过程通常有以下步骤：\n",
    "\n",
    "1. 从dataloader中取出一个batch data\n",
    "2. 将batch data喂给model，做前向计算\n",
    "3. 将前向计算结果传给损失函数，计算loss。将前向计算结果传给评价方法，计算评价指标。\n",
    "4. loss反向回传，更新梯度。重复以上步骤。\n",
    "\n",
    "每训练一个epoch时，程序将会评估一次，评估当前模型训练的效果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/math_op_patch.py:239: UserWarning: The dtype of left and right variables are not the same, left dtype is paddle.float32, but right dtype is paddle.bool, the right dtype will convert to paddle.float32\n",
      "  format(lhs_dtype, rhs_dtype, lhs_dtype))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 - step:1 - loss: 2.641742\n",
      "epoch:0 - step:2 - loss: 2.421458\n",
      "epoch:0 - step:3 - loss: 2.241805\n",
      "epoch:0 - step:4 - loss: 2.076534\n",
      "epoch:0 - step:5 - loss: 1.952870\n",
      "epoch:0 - step:6 - loss: 1.851481\n",
      "epoch:0 - step:7 - loss: 1.726657\n",
      "epoch:0 - step:8 - loss: 1.603177\n",
      "epoch:0 - step:9 - loss: 1.530140\n",
      "epoch:0 - step:10 - loss: 1.424838\n",
      "epoch:0 - step:11 - loss: 1.365694\n",
      "epoch:0 - step:12 - loss: 1.310175\n",
      "epoch:0 - step:13 - loss: 1.259889\n",
      "epoch:0 - step:14 - loss: 1.191792\n",
      "epoch:0 - step:15 - loss: 1.168801\n",
      "epoch:0 - step:16 - loss: 1.084729\n",
      "epoch:0 - step:17 - loss: 1.093034\n",
      "epoch:0 - step:18 - loss: 1.012313\n",
      "epoch:0 - step:19 - loss: 0.970749\n",
      "epoch:0 - step:20 - loss: 0.924451\n",
      "epoch:0 - step:21 - loss: 0.869113\n",
      "epoch:0 - step:22 - loss: 0.841398\n",
      "epoch:0 - step:23 - loss: 0.804191\n",
      "epoch:0 - step:24 - loss: 0.776479\n",
      "epoch:0 - step:25 - loss: 0.769694\n",
      "epoch:0 - step:26 - loss: 0.716212\n",
      "epoch:0 - step:27 - loss: 0.713279\n",
      "epoch:0 - step:28 - loss: 0.685468\n",
      "epoch:0 - step:29 - loss: 0.638174\n",
      "epoch:0 - step:30 - loss: 0.593973\n",
      "epoch:0 - step:31 - loss: 0.562339\n",
      "epoch:0 - step:32 - loss: 0.556730\n",
      "epoch:0 - step:33 - loss: 0.513264\n",
      "epoch:0 - step:34 - loss: 0.495494\n",
      "epoch:0 - step:35 - loss: 0.455093\n",
      "epoch:0 - step:36 - loss: 0.457316\n",
      "epoch:0 - step:37 - loss: 0.426923\n",
      "epoch:0 - step:38 - loss: 0.381468\n",
      "epoch:0 - step:39 - loss: 0.378602\n",
      "epoch:0 - step:40 - loss: 0.347566\n",
      "epoch:0 - step:41 - loss: 0.346385\n",
      "epoch:0 - step:42 - loss: 0.324733\n",
      "epoch:0 - step:43 - loss: 0.264631\n",
      "epoch:0 - step:44 - loss: 0.257997\n",
      "epoch:0 - step:45 - loss: 0.228233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-06-16 16:38:15,477] [ WARNING] - Compatibility Warning: The params of ChunkEvaluator.compute has been modified. The old version is `inputs`, `lengths`, `predictions`, `labels` while the current version is `lengths`, `predictions`, `labels`.  Please update the usage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval precision: 0.932190 - recall: 0.959630 - f1: 0.945711\n",
      "epoch:1 - step:46 - loss: 0.230952\n",
      "epoch:1 - step:47 - loss: 0.215577\n",
      "epoch:1 - step:48 - loss: 0.221225\n",
      "epoch:1 - step:49 - loss: 0.199531\n",
      "epoch:1 - step:50 - loss: 0.198224\n",
      "epoch:1 - step:51 - loss: 0.175371\n",
      "epoch:1 - step:52 - loss: 0.163069\n",
      "epoch:1 - step:53 - loss: 0.134598\n",
      "epoch:1 - step:54 - loss: 0.133064\n",
      "epoch:1 - step:55 - loss: 0.126076\n",
      "epoch:1 - step:56 - loss: 0.107437\n",
      "epoch:1 - step:57 - loss: 0.124397\n",
      "epoch:1 - step:58 - loss: 0.085090\n",
      "epoch:1 - step:59 - loss: 0.078551\n",
      "epoch:1 - step:60 - loss: 0.136946\n",
      "epoch:1 - step:61 - loss: 0.104542\n",
      "epoch:1 - step:62 - loss: 0.087848\n",
      "epoch:1 - step:63 - loss: 0.065619\n",
      "epoch:1 - step:64 - loss: 0.086226\n",
      "epoch:1 - step:65 - loss: 0.086361\n",
      "epoch:1 - step:66 - loss: 0.066396\n",
      "epoch:1 - step:67 - loss: 0.078656\n",
      "epoch:1 - step:68 - loss: 0.065434\n",
      "epoch:1 - step:69 - loss: 0.061247\n",
      "epoch:1 - step:70 - loss: 0.076129\n",
      "epoch:1 - step:71 - loss: 0.063647\n",
      "epoch:1 - step:72 - loss: 0.062907\n",
      "epoch:1 - step:73 - loss: 0.091930\n",
      "epoch:1 - step:74 - loss: 0.049207\n",
      "epoch:1 - step:75 - loss: 0.054690\n",
      "epoch:1 - step:76 - loss: 0.048743\n",
      "epoch:1 - step:77 - loss: 0.046201\n",
      "epoch:1 - step:78 - loss: 0.070466\n",
      "epoch:1 - step:79 - loss: 0.033788\n",
      "epoch:1 - step:80 - loss: 0.027996\n",
      "epoch:1 - step:81 - loss: 0.030946\n",
      "epoch:1 - step:82 - loss: 0.060975\n",
      "epoch:1 - step:83 - loss: 0.045090\n",
      "epoch:1 - step:84 - loss: 0.040483\n",
      "epoch:1 - step:85 - loss: 0.035978\n",
      "epoch:1 - step:86 - loss: 0.028713\n",
      "epoch:1 - step:87 - loss: 0.047399\n",
      "epoch:1 - step:88 - loss: 0.024541\n",
      "epoch:1 - step:89 - loss: 0.030144\n",
      "epoch:1 - step:90 - loss: 0.022773\n",
      "eval precision: 0.975833 - recall: 0.984861 - f1: 0.980326\n",
      "epoch:2 - step:91 - loss: 0.030242\n",
      "epoch:2 - step:92 - loss: 0.023636\n",
      "epoch:2 - step:93 - loss: 0.035363\n",
      "epoch:2 - step:94 - loss: 0.018900\n",
      "epoch:2 - step:95 - loss: 0.028251\n",
      "epoch:2 - step:96 - loss: 0.033088\n",
      "epoch:2 - step:97 - loss: 0.031439\n",
      "epoch:2 - step:98 - loss: 0.015068\n",
      "epoch:2 - step:99 - loss: 0.013974\n",
      "epoch:2 - step:100 - loss: 0.027766\n",
      "epoch:2 - step:101 - loss: 0.031027\n",
      "epoch:2 - step:102 - loss: 0.019165\n",
      "epoch:2 - step:103 - loss: 0.014296\n",
      "epoch:2 - step:104 - loss: 0.013616\n",
      "epoch:2 - step:105 - loss: 0.045370\n",
      "epoch:2 - step:106 - loss: 0.017941\n",
      "epoch:2 - step:107 - loss: 0.017512\n",
      "epoch:2 - step:108 - loss: 0.012482\n",
      "epoch:2 - step:109 - loss: 0.051014\n",
      "epoch:2 - step:110 - loss: 0.047656\n",
      "epoch:2 - step:111 - loss: 0.017817\n",
      "epoch:2 - step:112 - loss: 0.027766\n",
      "epoch:2 - step:113 - loss: 0.016241\n",
      "epoch:2 - step:114 - loss: 0.011875\n",
      "epoch:2 - step:115 - loss: 0.016028\n",
      "epoch:2 - step:116 - loss: 0.030052\n",
      "epoch:2 - step:117 - loss: 0.013545\n",
      "epoch:2 - step:118 - loss: 0.027675\n",
      "epoch:2 - step:119 - loss: 0.030948\n",
      "epoch:2 - step:120 - loss: 0.018104\n",
      "epoch:2 - step:121 - loss: 0.023824\n",
      "epoch:2 - step:122 - loss: 0.012099\n",
      "epoch:2 - step:123 - loss: 0.028877\n",
      "epoch:2 - step:124 - loss: 0.017460\n",
      "epoch:2 - step:125 - loss: 0.015511\n",
      "epoch:2 - step:126 - loss: 0.013618\n",
      "epoch:2 - step:127 - loss: 0.033579\n",
      "epoch:2 - step:128 - loss: 0.027739\n",
      "epoch:2 - step:129 - loss: 0.015810\n",
      "epoch:2 - step:130 - loss: 0.008155\n",
      "epoch:2 - step:131 - loss: 0.022728\n",
      "epoch:2 - step:132 - loss: 0.018272\n",
      "epoch:2 - step:133 - loss: 0.011756\n",
      "epoch:2 - step:134 - loss: 0.017999\n",
      "epoch:2 - step:135 - loss: 0.029228\n",
      "eval precision: 0.988265 - recall: 0.991590 - f1: 0.989924\n",
      "epoch:3 - step:136 - loss: 0.019713\n",
      "epoch:3 - step:137 - loss: 0.008225\n",
      "epoch:3 - step:138 - loss: 0.013367\n",
      "epoch:3 - step:139 - loss: 0.009795\n",
      "epoch:3 - step:140 - loss: 0.015044\n",
      "epoch:3 - step:141 - loss: 0.025991\n",
      "epoch:3 - step:142 - loss: 0.010725\n",
      "epoch:3 - step:143 - loss: 0.007942\n",
      "epoch:3 - step:144 - loss: 0.010801\n",
      "epoch:3 - step:145 - loss: 0.026387\n",
      "epoch:3 - step:146 - loss: 0.023336\n",
      "epoch:3 - step:147 - loss: 0.009346\n",
      "epoch:3 - step:148 - loss: 0.006305\n",
      "epoch:3 - step:149 - loss: 0.009439\n",
      "epoch:3 - step:150 - loss: 0.019108\n",
      "epoch:3 - step:151 - loss: 0.007444\n",
      "epoch:3 - step:152 - loss: 0.007398\n",
      "epoch:3 - step:153 - loss: 0.007026\n",
      "epoch:3 - step:154 - loss: 0.043161\n",
      "epoch:3 - step:155 - loss: 0.037427\n",
      "epoch:3 - step:156 - loss: 0.016376\n",
      "epoch:3 - step:157 - loss: 0.033929\n",
      "epoch:3 - step:158 - loss: 0.014221\n",
      "epoch:3 - step:159 - loss: 0.007433\n",
      "epoch:3 - step:160 - loss: 0.014236\n",
      "epoch:3 - step:161 - loss: 0.025169\n",
      "epoch:3 - step:162 - loss: 0.008115\n",
      "epoch:3 - step:163 - loss: 0.023159\n",
      "epoch:3 - step:164 - loss: 0.019613\n",
      "epoch:3 - step:165 - loss: 0.014989\n",
      "epoch:3 - step:166 - loss: 0.005793\n",
      "epoch:3 - step:167 - loss: 0.007087\n",
      "epoch:3 - step:168 - loss: 0.023218\n",
      "epoch:3 - step:169 - loss: 0.015656\n",
      "epoch:3 - step:170 - loss: 0.005488\n",
      "epoch:3 - step:171 - loss: 0.013503\n",
      "epoch:3 - step:172 - loss: 0.024658\n",
      "epoch:3 - step:173 - loss: 0.009922\n",
      "epoch:3 - step:174 - loss: 0.013065\n",
      "epoch:3 - step:175 - loss: 0.012942\n",
      "epoch:3 - step:176 - loss: 0.014493\n",
      "epoch:3 - step:177 - loss: 0.010686\n",
      "epoch:3 - step:178 - loss: 0.005711\n",
      "epoch:3 - step:179 - loss: 0.008164\n",
      "epoch:3 - step:180 - loss: 0.004899\n",
      "eval precision: 0.985786 - recall: 0.991590 - f1: 0.988679\n",
      "epoch:4 - step:181 - loss: 0.010467\n",
      "epoch:4 - step:182 - loss: 0.005399\n",
      "epoch:4 - step:183 - loss: 0.006039\n",
      "epoch:4 - step:184 - loss: 0.006831\n",
      "epoch:4 - step:185 - loss: 0.009133\n",
      "epoch:4 - step:186 - loss: 0.012717\n",
      "epoch:4 - step:187 - loss: 0.011799\n",
      "epoch:4 - step:188 - loss: 0.008865\n",
      "epoch:4 - step:189 - loss: 0.005324\n",
      "epoch:4 - step:190 - loss: 0.006486\n",
      "epoch:4 - step:191 - loss: 0.016254\n",
      "epoch:4 - step:192 - loss: 0.011148\n",
      "epoch:4 - step:193 - loss: 0.003845\n",
      "epoch:4 - step:194 - loss: 0.004352\n",
      "epoch:4 - step:195 - loss: 0.015657\n",
      "epoch:4 - step:196 - loss: 0.014940\n",
      "epoch:4 - step:197 - loss: 0.006426\n",
      "epoch:4 - step:198 - loss: 0.010431\n",
      "epoch:4 - step:199 - loss: 0.033077\n",
      "epoch:4 - step:200 - loss: 0.029602\n",
      "epoch:4 - step:201 - loss: 0.003986\n",
      "epoch:4 - step:202 - loss: 0.017657\n",
      "epoch:4 - step:203 - loss: 0.007405\n",
      "epoch:4 - step:204 - loss: 0.003508\n",
      "epoch:4 - step:205 - loss: 0.008946\n",
      "epoch:4 - step:206 - loss: 0.010420\n",
      "epoch:4 - step:207 - loss: 0.013400\n",
      "epoch:4 - step:208 - loss: 0.015091\n",
      "epoch:4 - step:209 - loss: 0.005868\n",
      "epoch:4 - step:210 - loss: 0.004145\n",
      "epoch:4 - step:211 - loss: 0.003945\n",
      "epoch:4 - step:212 - loss: 0.003687\n",
      "epoch:4 - step:213 - loss: 0.025927\n",
      "epoch:4 - step:214 - loss: 0.010526\n",
      "epoch:4 - step:215 - loss: 0.003111\n",
      "epoch:4 - step:216 - loss: 0.003868\n",
      "epoch:4 - step:217 - loss: 0.010613\n",
      "epoch:4 - step:218 - loss: 0.009014\n",
      "epoch:4 - step:219 - loss: 0.004153\n",
      "epoch:4 - step:220 - loss: 0.006160\n",
      "epoch:4 - step:221 - loss: 0.004251\n",
      "epoch:4 - step:222 - loss: 0.008032\n",
      "epoch:4 - step:223 - loss: 0.005566\n",
      "epoch:4 - step:224 - loss: 0.008829\n",
      "epoch:4 - step:225 - loss: 0.003573\n",
      "eval precision: 0.991604 - recall: 0.993272 - f1: 0.992437\n",
      "epoch:5 - step:226 - loss: 0.007671\n",
      "epoch:5 - step:227 - loss: 0.004103\n",
      "epoch:5 - step:228 - loss: 0.003933\n",
      "epoch:5 - step:229 - loss: 0.004418\n",
      "epoch:5 - step:230 - loss: 0.005725\n",
      "epoch:5 - step:231 - loss: 0.006150\n",
      "epoch:5 - step:232 - loss: 0.014980\n",
      "epoch:5 - step:233 - loss: 0.002877\n",
      "epoch:5 - step:234 - loss: 0.002902\n",
      "epoch:5 - step:235 - loss: 0.002957\n",
      "epoch:5 - step:236 - loss: 0.003021\n",
      "epoch:5 - step:237 - loss: 0.003823\n",
      "epoch:5 - step:238 - loss: 0.002620\n",
      "epoch:5 - step:239 - loss: 0.002687\n",
      "epoch:5 - step:240 - loss: 0.010243\n",
      "epoch:5 - step:241 - loss: 0.006721\n",
      "epoch:5 - step:242 - loss: 0.003398\n",
      "epoch:5 - step:243 - loss: 0.003988\n",
      "epoch:5 - step:244 - loss: 0.036990\n",
      "epoch:5 - step:245 - loss: 0.023918\n",
      "epoch:5 - step:246 - loss: 0.003473\n",
      "epoch:5 - step:247 - loss: 0.007155\n",
      "epoch:5 - step:248 - loss: 0.003824\n",
      "epoch:5 - step:249 - loss: 0.002872\n",
      "epoch:5 - step:250 - loss: 0.006037\n",
      "epoch:5 - step:251 - loss: 0.006598\n",
      "epoch:5 - step:252 - loss: 0.009694\n",
      "epoch:5 - step:253 - loss: 0.005989\n",
      "epoch:5 - step:254 - loss: 0.003967\n",
      "epoch:5 - step:255 - loss: 0.002623\n",
      "epoch:5 - step:256 - loss: 0.002598\n",
      "epoch:5 - step:257 - loss: 0.006062\n",
      "epoch:5 - step:258 - loss: 0.023867\n",
      "epoch:5 - step:259 - loss: 0.004247\n",
      "epoch:5 - step:260 - loss: 0.002647\n",
      "epoch:5 - step:261 - loss: 0.002560\n",
      "epoch:5 - step:262 - loss: 0.013286\n",
      "epoch:5 - step:263 - loss: 0.008507\n",
      "epoch:5 - step:264 - loss: 0.004134\n",
      "epoch:5 - step:265 - loss: 0.005125\n",
      "epoch:5 - step:266 - loss: 0.002592\n",
      "epoch:5 - step:267 - loss: 0.008569\n",
      "epoch:5 - step:268 - loss: 0.002621\n",
      "epoch:5 - step:269 - loss: 0.005644\n",
      "epoch:5 - step:270 - loss: 0.002638\n",
      "eval precision: 0.992443 - recall: 0.994113 - f1: 0.993277\n",
      "epoch:6 - step:271 - loss: 0.003266\n",
      "epoch:6 - step:272 - loss: 0.007793\n",
      "epoch:6 - step:273 - loss: 0.005965\n",
      "epoch:6 - step:274 - loss: 0.004013\n",
      "epoch:6 - step:275 - loss: 0.004499\n",
      "epoch:6 - step:276 - loss: 0.003254\n",
      "epoch:6 - step:277 - loss: 0.003865\n",
      "epoch:6 - step:278 - loss: 0.002797\n",
      "epoch:6 - step:279 - loss: 0.002372\n",
      "epoch:6 - step:280 - loss: 0.002193\n",
      "epoch:6 - step:281 - loss: 0.005777\n",
      "epoch:6 - step:282 - loss: 0.004366\n",
      "epoch:6 - step:283 - loss: 0.001958\n",
      "epoch:6 - step:284 - loss: 0.002141\n",
      "epoch:6 - step:285 - loss: 0.007166\n",
      "epoch:6 - step:286 - loss: 0.023343\n",
      "epoch:6 - step:287 - loss: 0.012312\n",
      "epoch:6 - step:288 - loss: 0.004039\n",
      "epoch:6 - step:289 - loss: 0.014287\n",
      "epoch:6 - step:290 - loss: 0.034247\n",
      "epoch:6 - step:291 - loss: 0.009001\n",
      "epoch:6 - step:292 - loss: 0.004746\n",
      "epoch:6 - step:293 - loss: 0.002203\n",
      "epoch:6 - step:294 - loss: 0.001985\n",
      "epoch:6 - step:295 - loss: 0.006354\n",
      "epoch:6 - step:296 - loss: 0.009502\n",
      "epoch:6 - step:297 - loss: 0.002475\n",
      "epoch:6 - step:298 - loss: 0.007594\n",
      "epoch:6 - step:299 - loss: 0.003731\n",
      "epoch:6 - step:300 - loss: 0.002722\n",
      "epoch:6 - step:301 - loss: 0.004825\n",
      "epoch:6 - step:302 - loss: 0.005313\n",
      "epoch:6 - step:303 - loss: 0.024428\n",
      "epoch:6 - step:304 - loss: 0.008226\n",
      "epoch:6 - step:305 - loss: 0.001979\n",
      "epoch:6 - step:306 - loss: 0.002089\n",
      "epoch:6 - step:307 - loss: 0.003642\n",
      "epoch:6 - step:308 - loss: 0.009762\n",
      "epoch:6 - step:309 - loss: 0.004080\n",
      "epoch:6 - step:310 - loss: 0.003817\n",
      "epoch:6 - step:311 - loss: 0.004004\n",
      "epoch:6 - step:312 - loss: 0.004450\n",
      "epoch:6 - step:313 - loss: 0.001975\n",
      "epoch:6 - step:314 - loss: 0.006559\n",
      "epoch:6 - step:315 - loss: 0.002071\n",
      "eval precision: 0.990772 - recall: 0.993272 - f1: 0.992020\n",
      "epoch:7 - step:316 - loss: 0.004409\n",
      "epoch:7 - step:317 - loss: 0.001971\n",
      "epoch:7 - step:318 - loss: 0.002577\n",
      "epoch:7 - step:319 - loss: 0.003528\n",
      "epoch:7 - step:320 - loss: 0.001911\n",
      "epoch:7 - step:321 - loss: 0.004136\n",
      "epoch:7 - step:322 - loss: 0.002153\n",
      "epoch:7 - step:323 - loss: 0.001698\n",
      "epoch:7 - step:324 - loss: 0.001840\n",
      "epoch:7 - step:325 - loss: 0.001913\n",
      "epoch:7 - step:326 - loss: 0.003878\n",
      "epoch:7 - step:327 - loss: 0.002158\n",
      "epoch:7 - step:328 - loss: 0.001824\n",
      "epoch:7 - step:329 - loss: 0.001645\n",
      "epoch:7 - step:330 - loss: 0.004229\n",
      "epoch:7 - step:331 - loss: 0.003351\n",
      "epoch:7 - step:332 - loss: 0.001817\n",
      "epoch:7 - step:333 - loss: 0.001723\n",
      "epoch:7 - step:334 - loss: 0.003511\n",
      "epoch:7 - step:335 - loss: 0.024213\n",
      "epoch:7 - step:336 - loss: 0.005118\n",
      "epoch:7 - step:337 - loss: 0.002969\n",
      "epoch:7 - step:338 - loss: 0.002705\n",
      "epoch:7 - step:339 - loss: 0.001843\n",
      "epoch:7 - step:340 - loss: 0.004203\n",
      "epoch:7 - step:341 - loss: 0.006567\n",
      "epoch:7 - step:342 - loss: 0.004071\n",
      "epoch:7 - step:343 - loss: 0.004093\n",
      "epoch:7 - step:344 - loss: 0.017686\n",
      "epoch:7 - step:345 - loss: 0.001618\n",
      "epoch:7 - step:346 - loss: 0.002012\n",
      "epoch:7 - step:347 - loss: 0.001644\n",
      "epoch:7 - step:348 - loss: 0.022313\n",
      "epoch:7 - step:349 - loss: 0.003276\n",
      "epoch:7 - step:350 - loss: 0.001463\n",
      "epoch:7 - step:351 - loss: 0.001565\n",
      "epoch:7 - step:352 - loss: 0.001829\n",
      "epoch:7 - step:353 - loss: 0.001654\n",
      "epoch:7 - step:354 - loss: 0.002443\n",
      "epoch:7 - step:355 - loss: 0.003272\n",
      "epoch:7 - step:356 - loss: 0.002185\n",
      "epoch:7 - step:357 - loss: 0.004926\n",
      "epoch:7 - step:358 - loss: 0.002377\n",
      "epoch:7 - step:359 - loss: 0.002002\n",
      "epoch:7 - step:360 - loss: 0.001675\n",
      "eval precision: 0.991604 - recall: 0.993272 - f1: 0.992437\n",
      "epoch:8 - step:361 - loss: 0.001490\n",
      "epoch:8 - step:362 - loss: 0.003839\n",
      "epoch:8 - step:363 - loss: 0.003197\n",
      "epoch:8 - step:364 - loss: 0.001742\n",
      "epoch:8 - step:365 - loss: 0.001560\n",
      "epoch:8 - step:366 - loss: 0.001733\n",
      "epoch:8 - step:367 - loss: 0.001707\n",
      "epoch:8 - step:368 - loss: 0.001442\n",
      "epoch:8 - step:369 - loss: 0.001667\n",
      "epoch:8 - step:370 - loss: 0.001654\n",
      "epoch:8 - step:371 - loss: 0.010325\n",
      "epoch:8 - step:372 - loss: 0.003572\n",
      "epoch:8 - step:373 - loss: 0.001436\n",
      "epoch:8 - step:374 - loss: 0.001480\n",
      "epoch:8 - step:375 - loss: 0.002347\n",
      "epoch:8 - step:376 - loss: 0.003170\n",
      "epoch:8 - step:377 - loss: 0.001354\n",
      "epoch:8 - step:378 - loss: 0.002847\n",
      "epoch:8 - step:379 - loss: 0.004105\n",
      "epoch:8 - step:380 - loss: 0.022444\n",
      "epoch:8 - step:381 - loss: 0.001454\n",
      "epoch:8 - step:382 - loss: 0.003494\n",
      "epoch:8 - step:383 - loss: 0.001338\n",
      "epoch:8 - step:384 - loss: 0.001288\n",
      "epoch:8 - step:385 - loss: 0.003119\n",
      "epoch:8 - step:386 - loss: 0.003312\n",
      "epoch:8 - step:387 - loss: 0.001353\n",
      "epoch:8 - step:388 - loss: 0.001527\n",
      "epoch:8 - step:389 - loss: 0.002958\n",
      "epoch:8 - step:390 - loss: 0.001856\n",
      "epoch:8 - step:391 - loss: 0.001448\n",
      "epoch:8 - step:392 - loss: 0.003905\n",
      "epoch:8 - step:393 - loss: 0.022981\n",
      "epoch:8 - step:394 - loss: 0.001273\n",
      "epoch:8 - step:395 - loss: 0.001943\n",
      "epoch:8 - step:396 - loss: 0.001531\n",
      "epoch:8 - step:397 - loss: 0.018255\n",
      "epoch:8 - step:398 - loss: 0.006474\n",
      "epoch:8 - step:399 - loss: 0.004484\n",
      "epoch:8 - step:400 - loss: 0.001234\n",
      "epoch:8 - step:401 - loss: 0.001596\n",
      "epoch:8 - step:402 - loss: 0.002212\n",
      "epoch:8 - step:403 - loss: 0.001756\n",
      "epoch:8 - step:404 - loss: 0.001751\n",
      "epoch:8 - step:405 - loss: 0.001376\n",
      "eval precision: 0.993277 - recall: 0.994113 - f1: 0.993695\n",
      "epoch:9 - step:406 - loss: 0.001484\n",
      "epoch:9 - step:407 - loss: 0.003089\n",
      "epoch:9 - step:408 - loss: 0.005219\n",
      "epoch:9 - step:409 - loss: 0.001639\n",
      "epoch:9 - step:410 - loss: 0.001649\n",
      "epoch:9 - step:411 - loss: 0.002382\n",
      "epoch:9 - step:412 - loss: 0.001291\n",
      "epoch:9 - step:413 - loss: 0.001412\n",
      "epoch:9 - step:414 - loss: 0.001594\n",
      "epoch:9 - step:415 - loss: 0.001613\n",
      "epoch:9 - step:416 - loss: 0.002349\n",
      "epoch:9 - step:417 - loss: 0.001645\n",
      "epoch:9 - step:418 - loss: 0.001171\n",
      "epoch:9 - step:419 - loss: 0.001275\n",
      "epoch:9 - step:420 - loss: 0.009009\n",
      "epoch:9 - step:421 - loss: 0.001238\n",
      "epoch:9 - step:422 - loss: 0.001222\n",
      "epoch:9 - step:423 - loss: 0.003758\n",
      "epoch:9 - step:424 - loss: 0.001492\n",
      "epoch:9 - step:425 - loss: 0.026069\n",
      "epoch:9 - step:426 - loss: 0.001233\n",
      "epoch:9 - step:427 - loss: 0.004515\n",
      "epoch:9 - step:428 - loss: 0.001274\n",
      "epoch:9 - step:429 - loss: 0.001180\n",
      "epoch:9 - step:430 - loss: 0.003707\n",
      "epoch:9 - step:431 - loss: 0.001543\n",
      "epoch:9 - step:432 - loss: 0.001483\n",
      "epoch:9 - step:433 - loss: 0.002676\n",
      "epoch:9 - step:434 - loss: 0.001951\n",
      "epoch:9 - step:435 - loss: 0.004291\n",
      "epoch:9 - step:436 - loss: 0.001895\n",
      "epoch:9 - step:437 - loss: 0.002024\n",
      "epoch:9 - step:438 - loss: 0.019194\n",
      "epoch:9 - step:439 - loss: 0.001300\n",
      "epoch:9 - step:440 - loss: 0.002058\n",
      "epoch:9 - step:441 - loss: 0.001068\n",
      "epoch:9 - step:442 - loss: 0.003240\n",
      "epoch:9 - step:443 - loss: 0.006167\n",
      "epoch:9 - step:444 - loss: 0.004796\n",
      "epoch:9 - step:445 - loss: 0.002957\n",
      "epoch:9 - step:446 - loss: 0.007042\n",
      "epoch:9 - step:447 - loss: 0.002480\n",
      "epoch:9 - step:448 - loss: 0.001538\n",
      "epoch:9 - step:449 - loss: 0.001326\n",
      "epoch:9 - step:450 - loss: 0.001084\n",
      "eval precision: 0.993277 - recall: 0.994113 - f1: 0.993695\n"
     ]
    }
   ],
   "source": [
    "step = 0\n",
    "for epoch in range(10):\n",
    "    for idx, (input_ids, token_type_ids, length, labels) in enumerate(train_loader):\n",
    "        logits = model(input_ids, token_type_ids)\n",
    "        loss = paddle.mean(loss_fn(logits, labels))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.clear_grad()\n",
    "        step += 1\n",
    "        print(\"epoch:%d - step:%d - loss: %f\" % (epoch, step, loss))\n",
    "    evaluate(model, metric, dev_loader)\n",
    "\n",
    "    paddle.save(model.state_dict(),\n",
    "                './ernie_result/model_%d.pdparams' % step)\n",
    "# model.save_pretrained('./checkpoint')\n",
    "# tokenizer.save_pretrained('./checkpoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import time\r\n",
    "# global_step = 0\r\n",
    "# last_step = 10 * len(train_data_loader)\r\n",
    "# tic_train = time.time()\r\n",
    "# for epoch in range(10):\r\n",
    "#         for step, batch in enumerate(train_data_loader):\r\n",
    "#             global_step += 1\r\n",
    "#             input_ids, token_type_ids, _, labels = batch\r\n",
    "#             logits = model(input_ids, token_type_ids)\r\n",
    "#             loss = loss_fct(logits, labels)\r\n",
    "#             avg_loss = paddle.mean(loss)\r\n",
    "#             if global_step % args.logging_steps == 0:\r\n",
    "#                 print(\r\n",
    "#                     \"global step %d, epoch: %d, batch: %d, loss: %f, speed: %.2f step/s\"\r\n",
    "#                     % (global_step, epoch, step, avg_loss,\r\n",
    "#                        args.logging_steps / (time.time() - tic_train)))\r\n",
    "#                 tic_train = time.time()\r\n",
    "#             avg_loss.backward()\r\n",
    "#             optimizer.step()\r\n",
    "#             lr_scheduler.step()\r\n",
    "#             optimizer.clear_grad()\r\n",
    "#             if global_step % args.save_steps == 0 or global_step == last_step:\r\n",
    "#                 if paddle.distributed.get_rank() == 0:\r\n",
    "#                     evaluate(model, loss_fct, metric, test_data_loader,\r\n",
    "#                              label_num)\r\n",
    "#                     paddle.save(model.state_dict(),\r\n",
    "#                                 os.path.join(args.output_dir,\r\n",
    "#                                              \"model_%d.pdparams\" % global_step))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# criterion = paddle.nn.loss.CrossEntropyLoss()\r\n",
    "# step = 0\r\n",
    "# for epoch in range(10):\r\n",
    "#     for idx, (input_ids, token_type_ids, length, labels) in enumerate(train_loader):\r\n",
    "#         logits = model(input_ids, token_type_ids)\r\n",
    "#         loss = criterion(logits, labels)\r\n",
    "#         probs = paddle.nn.functional.softmax(logits, axis=1)\r\n",
    "#         loss.backward()\r\n",
    "#         optimizer.step()\r\n",
    "#         optimizer.clear_grad()\r\n",
    "#         step += 1\r\n",
    "#         print(\"epoch:%d - step:%d - loss: %f\" % (epoch, step, loss))\r\n",
    "#     evaluate(model, metric, dev_loader)\r\n",
    "\r\n",
    "#     paddle.save(model.state_dict(),\r\n",
    "#                 './ernie_result/model_%d.pdparams' % step)\r\n",
    "# model.save_pretrained('./checkpoint')\r\n",
    "# tokenizer.save_pretrained('./checkpoint')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 模型预测\n",
    "\n",
    "训练保存好的模型，即可用于预测。如以下示例代码自定义预测数据，调用`predict()`函数即可一键预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results have been saved in the file: ernie_results.txt, some examples are shown below: \n",
      "('黑龙江省', 'A1')('双鸭山市', 'A2')('尖山区', 'A3')('八马路与东平行路交叉口北40米', 'A4')('韦业涛', 'P')('18600009172', 'T')\n",
      "('广西壮族自治区', 'A1')('桂林市', 'A2')('雁山区', 'A3')('雁山镇西龙村老年活动中心', 'A4')('17610348888', 'T')('羊卓卫', 'P')\n",
      "('15652864561', 'T')('河南省', 'A1')('开封市', 'A2')('顺河回族区', 'A3')('顺河区公园路32号', 'A4')('赵本山', 'P')\n",
      "('河北省', 'A1')('唐山市', 'A2')('玉田县', 'A3')('无终大街159号', 'A4')('18614253058', 'T')('尚汉生', 'P')\n",
      "('台湾', 'A1')('台中市', 'A2')('北区', 'A3')('北区锦新街18号', 'A4')('18511226708', 'T')('蓟丽', 'P')\n",
      "('廖梓琪', 'P')('18514743222', 'T')('湖北省', 'A1')('宜昌市', 'A2')('长阳土家族自治县', 'A3')('贺家坪镇贺家坪村一组临河1号', 'A4')\n",
      "('江苏省', 'A1')('南通市', 'A2')('海门市', 'A3')('孝威村孝威路88号', 'A4')('18611840623', 'T')('计星仪', 'P')\n",
      "('17601674746', 'T')('赵春丽', 'P')('内蒙古自治区', 'A1')('乌兰察布市', 'A2')('凉城县', 'A3')('新建街', 'A4')\n",
      "('云南省', 'A1')('临沧市', 'A2')('耿马傣族佤族自治县', 'A3')('鑫源路法院对面', 'A4')('许贞爱', 'P')('18510566685', 'T')\n",
      "('四川省', 'A1')('成都市', 'A2')('双流区', 'A3')('东升镇北仓路196号', 'A4')('耿丕岭', 'P')('18513466161', 'T')\n"
     ]
    }
   ],
   "source": [
    "preds = predict(model, test_loader, test_ds, label_vocab)\n",
    "file_path = \"ernie_results.txt\"\n",
    "with open(file_path, \"w\", encoding=\"utf8\") as fout:\n",
    "    fout.write(\"\\n\".join(preds))\n",
    "# Print some examples\n",
    "print(\n",
    "    \"The results have been saved in the file: %s, some examples are shown below: \"\n",
    "    % file_path)\n",
    "print(\"\\n\".join(preds[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 进一步使用CRF\n",
    "\n",
    "PaddleNLP提供了CRF Layer，它能够学习label之间的关系，能够帮助模型更好地学习、预测序列标注任务。\n",
    "\n",
    "我们在PaddleNLP仓库中提供了[示例](https://github.com/PaddlePaddle/PaddleNLP/blob/develop/examples/information_extraction/waybill_ie/run_ernie_crf.py)，您可以参照示例代码使用Ernie-CRF结构完成快递单信息抽取任务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import paddle.nn as nn\r\n",
    "from paddlenlp.transformers import ErniePretrainedModel\r\n",
    "from paddlenlp.layers.crf import LinearChainCrf, ViterbiDecoder, LinearChainCrfLoss\r\n",
    "\r\n",
    "\r\n",
    "class ErnieCrfForTokenClassification(nn.Layer):\r\n",
    "    def __init__(self, ernie, crf_lr=100):\r\n",
    "        super().__init__()\r\n",
    "        self.num_classes = ernie.num_classes\r\n",
    "        self.ernie = ernie  # allow ernie to be config\r\n",
    "        self.crf = LinearChainCrf(\r\n",
    "            self.num_classes, crf_lr=crf_lr, with_start_stop_tag=False)\r\n",
    "        self.crf_loss = LinearChainCrfLoss(self.crf)\r\n",
    "        self.viterbi_decoder = ViterbiDecoder(\r\n",
    "            self.crf.transitions, with_start_stop_tag=False)\r\n",
    "\r\n",
    "    def forward(self,\r\n",
    "                input_ids,\r\n",
    "                token_type_ids=None,\r\n",
    "                position_ids=None,\r\n",
    "                attention_mask=None,\r\n",
    "                lengths=None,\r\n",
    "                labels=None):\r\n",
    "        logits = self.ernie(\r\n",
    "            input_ids,\r\n",
    "            token_type_ids=token_type_ids,\r\n",
    "            attention_mask=attention_mask,\r\n",
    "            position_ids=position_ids)\r\n",
    "\r\n",
    "        if labels is not None:\r\n",
    "            loss = self.crf_loss(logits, lengths, labels)\r\n",
    "            return loss\r\n",
    "        else:\r\n",
    "            _, prediction = self.viterbi_decoder(logits, lengths)\r\n",
    "            return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ernie = ErnieForTokenClassification.from_pretrained(\r\n",
    "        \"ernie-1.0\", num_classes=len(label_vocab))\r\n",
    "model = ErnieCrfForTokenClassification(ernie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "metric = ChunkEvaluator(label_list=label_vocab.keys(), suffix=True)\r\n",
    "optimizer = paddle.optimizer.AdamW(\r\n",
    "        learning_rate=2e-5, parameters=model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "step = 0\r\n",
    "for epoch in range(10):\r\n",
    "    for input_ids, token_type_ids, lengths, labels in train_loader:\r\n",
    "        loss = model(\r\n",
    "                input_ids, token_type_ids, lengths=lengths, labels=labels)\r\n",
    "        avg_loss = paddle.mean(loss)\r\n",
    "        avg_loss.backward()\r\n",
    "        optimizer.step()\r\n",
    "        optimizer.clear_grad()\r\n",
    "        step += 1\r\n",
    "        print(\"[TRAIN] Epoch:%d - Step:%d - Loss: %f\" %\r\n",
    "                  (epoch, step, avg_loss))\r\n",
    "    evaluate(model, metric, dev_loader)\r\n",
    "\r\n",
    "    paddle.save(model.state_dict(),\r\n",
    "                    './ernie_crf_ckpt/model_%d.pdparams' % step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preds = predict(model, test_loader, test_ds, label_vocab)\r\n",
    "# file_path = \"ernie_crf_results.txt\"\r\n",
    "file_path = \"ernie_results.txt\"\r\n",
    "with open(file_path, \"w\", encoding=\"utf8\") as fout:\r\n",
    "    fout.write(\"\\n\".join(preds))\r\n",
    "    # Print some examples\r\n",
    "print(\"The results have been saved in the file: %s, some examples are shown below: \"\r\n",
    "        % file_path)\r\n",
    "print(\"\\n\".join(preds[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 加入交流群，一起学习吧\n",
    "\n",
    "现在就加入课程QQ交流群，一起交流NLP技术吧！\n",
    "\n",
    "<img src=\"https://ai-studio-static-online.cdn.bcebos.com/d953727af0c24a7c806ab529495f0904f22f809961be420b8c88cdf59b837394\" width=\"200\" height=\"250\" >"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.1.0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
